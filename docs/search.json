[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "üë®üèª‚Äçüéì Bacharelado em Engenharia Qu√≠mica | Universidade Estadual de Maring√° - UEM | 2016-2020.\nüèÖ Especialista em Intelig√™ncia Artificial e Computacional | Universidade Federal de Vi√ßosa - UFV | 2023.\nüî≠ Atualmente, trabalho como Cientista de Dados, focado na constru√ß√£o de modelos preditivos para o agroneg√≥cio.\nüå± Busco aprimoramento constante de acordo com as melhores pr√°ticas de MLOps.\nüí¨ Podemos discutir sobre todo o ciclo de vida de modelos de Aprendizado de M√°quina.\nüòÑ Pronomes: Ele/Dele.\n‚ö° Curiosidade: Conclu√≠ minha gradua√ß√£o em Engenharia Qu√≠mica no final de 2020. Em seguida, iniciei minha jornada na ind√∫stria qu√≠mica no setor de P&D. No entanto, minha paix√£o por tecnologia e an√°lise de dados come√ßou a falar mais alto (uso programa√ß√£o de computadores desde 2016, resolvendo problemas de engenharia). Decidi, ent√£o, ingressar no mundo da Ci√™ncia de Dados, come√ßando na √°rea de BI/BA e, posteriormente, atuando como Cientista de Dados com foco no desenvolvimento e implementa√ß√£o de modelos de Aprendizado de M√°quina."
  },
  {
    "objectID": "index.html#ol√°-seja-bem-vindo-ao-meu-portf√≥lio.",
    "href": "index.html#ol√°-seja-bem-vindo-ao-meu-portf√≥lio.",
    "title": "Home",
    "section": "",
    "text": "üë®üèª‚Äçüéì Bacharelado em Engenharia Qu√≠mica | Universidade Estadual de Maring√° - UEM | 2016-2020.\nüèÖ Especialista em Intelig√™ncia Artificial e Computacional | Universidade Federal de Vi√ßosa - UFV | 2023.\nüî≠ Atualmente, trabalho como Cientista de Dados, focado na constru√ß√£o de modelos preditivos para o agroneg√≥cio.\nüå± Busco aprimoramento constante de acordo com as melhores pr√°ticas de MLOps.\nüí¨ Podemos discutir sobre todo o ciclo de vida de modelos de Aprendizado de M√°quina.\nüòÑ Pronomes: Ele/Dele.\n‚ö° Curiosidade: Conclu√≠ minha gradua√ß√£o em Engenharia Qu√≠mica no final de 2020. Em seguida, iniciei minha jornada na ind√∫stria qu√≠mica no setor de P&D. No entanto, minha paix√£o por tecnologia e an√°lise de dados come√ßou a falar mais alto (uso programa√ß√£o de computadores desde 2016, resolvendo problemas de engenharia). Decidi, ent√£o, ingressar no mundo da Ci√™ncia de Dados, come√ßando na √°rea de BI/BA e, posteriormente, atuando como Cientista de Dados com foco no desenvolvimento e implementa√ß√£o de modelos de Aprendizado de M√°quina."
  },
  {
    "objectID": "index.html#ferramentas.",
    "href": "index.html#ferramentas.",
    "title": "Home",
    "section": "Ferramentas.",
    "text": "Ferramentas."
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference",
    "section": "",
    "text": "Livro 1\n\n\nLivro 2\n\n\nLivro 3"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "",
    "text": "O mercado imobili√°rio √© uma fonte rica de informa√ß√µes sobre propriedades dispon√≠veis para venda ou aluguel. No entanto, navegar por in√∫meros an√∫ncios em diferentes plataformas pode ser uma tarefa √°rdua e demorada. √â aqui que entra o web scraping, uma t√©cnica poderosa para extrair dados de maneira automatizada e eficiente.\n\n\nNeste projeto, embarcaremos em uma jornada para extrair informa√ß√µes de im√≥veis do renomado site Viva Real https://www.vivareal.com.br/.\n\n\nUsando BeautifulSoup em Python, iremos explorar a estrutura HTML das p√°ginas do Viva Real, navegar pelos elementos relevantes e extrair detalhes importantes, como pre√ßo, localiza√ß√£o, caracter√≠sticas do im√≥vel e informa√ß√µes sobre o anunciante."
  },
  {
    "objectID": "project4.html#setup",
    "href": "project4.html#setup",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Setup:",
    "text": "Setup:\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n# Imports\nfrom   bs4 import BeautifulSoup\nimport numpy  as np\nimport pandas as pd\nimport requests\nimport time"
  },
  {
    "objectID": "project4.html#dados-que-vamos-extrair",
    "href": "project4.html#dados-que-vamos-extrair",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Dados que vamos extrair:",
    "text": "Dados que vamos extrair:\nCriando as listas onde vamos armazenar os dados raspados.\n\n# Lists\nfull_adress      = []\nfull_title       = []\nfull_area        = []\nfull_area_unit   = []\nfull_room        = []\nfull_bath        = []\nfull_garage      = []\nfull_price       = []\nfull_price_condo = []\nfull_publisher   = []"
  },
  {
    "objectID": "project4.html#scraping",
    "href": "project4.html#scraping",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Scraping",
    "text": "Scraping\n\nEstamos interessados em extrair informa√ß√µes de todos os im√≥veis dispon√≠veis para loca√ß√£o na cidade de Jaguari√∫na - SP.\n\n\nEm 03/05/2024, esses im√≥veis est√£o distribu√≠dos em 17 p√°ginas, totalizando 612.\n\n\nPara cada im√≥vel, ainda teremos que acessar uma p√°gina extra para conseguirmos coletar o nome da imobili√°ria anunciante, portanto, vamos raspar um total de 17 + 612 = 629 p√°ginas.\n\n\nDesta forma, sempre que fizermos uma requisi√ß√£o em uma p√°gina, vamos esperar 10 segundos para fazer a pr√≥xima, a fim de n√£o sobrecarregar o servidor.\n\n\nt = 10 \n\n# Pages\nfor p in range(1,18):\n    \n    print(f\"pagina: {p}\")\n    \n    time.sleep(t)   \n    \n    url     = f\"https://www.vivareal.com.br/aluguel/sp/jaguariuna?page={p}\"\n    request = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0\"})\n    soup    = BeautifulSoup(request.text, \"html.parser\")\n    \n\n    # Imo\n    builds  = soup.findAll(class_ = \"property-card__content-link js-card-title\")\n\n    # link\n    link_builds = soup.findAll(\"a\", attrs={\"class\": \"property-card__labels-container js-main-info js-listing-labels-link\"})\n    url_link = [\"https://www.vivareal.com.br\" + link[\"href\"] for link in link_builds]\n    \n    \n    i = 0\n    for build, link in zip(builds, url_link):\n        \n        i += 1\n        print(f\"imo: {i}\")\n            \n    \n        try:\n            adress = build.find(\"span\", attrs={\"class\": \"property-card__address\"}).text\n            full_adress.append(adress.strip())\n        except:\n            full_adress.append(\"VAZIO\")\n    \n        try:\n            title = build.find(\"span\", attrs={\"class\": \"property-card__title js-cardLink js-card-title\"}).text\n            full_title.append(title.strip())\n        except:\n            full_title.append(\"VAZIO\")\n    \n        try:\n            area = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-area\"}).text.split()\n            full_area.append(area[0].strip())\n        except:\n            full_area.append(\"0\")\n    \n        try:\n            area = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-area\"}).text.split()\n            full_area_unit.append(area[1].strip())\n        except:\n            full_area_unit.append(\"0\")\n    \n        try:\n            room = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-room js-property-detail-rooms\"}).text.split()\n            full_room.append(room[0].strip())\n        except:\n            full_room.append(\"0\")\n    \n        try:\n            bath = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom\"}).text.split()\n            full_bath.append(bath[0].strip())\n        except:\n            full_bath.append(\"0\")\n    \n        try:\n            garage = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-garage js-property-detail-garages\"}).text.split()\n            full_garage.append(garage[0].strip())\n        except:\n            full_garage.append(\"0\")\n    \n        try:\n            price = build.find(\"div\", attrs={\"class\": \"property-card__price js-property-card-prices js-property-card__price-small\"}).text\n            full_price.append(price.strip())\n        except:\n            full_price.append(\"0\")\n    \n        try:\n            price_condo = build.find(\"div\", attrs={\"class\": \"property-card__price-details--condo\"}).text.split()\n            full_price_condo.append(price_condo[-1].strip())\n        except:\n            full_price_condo.append(\"0\")\n            \n        time.sleep(t)\n        try:\n            request2   = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"})\n            soup2      = BeautifulSoup(request2.text, \"html.parser\")\n            publisher = soup2.find(\"a\", attrs={\"class\": \"publisher-details__name\"})\n            full_publisher.append( publisher.text.strip())\n        except:\n            full_publisher.append('0')"
  },
  {
    "objectID": "project4.html#salvando-o-resultado",
    "href": "project4.html#salvando-o-resultado",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Salvando o resultado",
    "text": "Salvando o resultado\nSalvando os dados em um dataframe do Pandas\n\ndf_from_web_scraping = pd.DataFrame(\n    columns = [\n        \"Endere√ßo\", \n        \"T√≠tulo\", \n        \"√Årea\", \n        \"Und. medida √°rea\", \n        \"Qtd. quartos\", \n        \"Qtd. banheiros\", \n        \"Qtd. vaga garagem\", \n        \"Pre√ßo alugel\", \n        \"Pre√ßo condom√≠nio\",\n        \"Anunciante\"\n    ],\n    data   = np.array([      \n        full_adress,\n        full_title,      \n        full_area,       \n        full_area_unit,  \n        full_room,       \n        full_bath,       \n        full_garage,     \n        full_price,      \n        full_price_condo,\n        full_publisher\n    ]).T\n)\n\ndf_from_web_scraping.to_pickle(\"source/imoveis.pkl\")\ndf_from_web_scraping.to_excel(\"source/imoveis.xlsx\", index = False)"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "",
    "text": "Devido aos avan√ßos da IA Generativa com os LLMs, o ChatGPT, desenvolvido pela OpenAI, tornou-se uma ferramenta popular para gera√ß√£o de texto. Atrav√©s da interface em https://chat.openai.com/, os usu√°rios podem interagir com o ChatGPT para receber respostas, resumos de texto e tradu√ß√µes. No entanto, para responder a perguntas espec√≠ficas de contexto in√©dito, como as baseadas em um documento PDF, √© geralmente necess√°rio usar a API do ChatGPT.\n\n\nPara simplificar essa tarefa, surgiu o LangChain https://www.langchain.com/, uma estrutura de c√≥digo aberto que facilita o desenvolvimento de aplicativos com LLMs. O LangChain atua como uma interface gen√©rica para diferentes LLMs, permitindo a constru√ß√£o de aplicativos LLM integrados a fontes de dados externas.\n\n\nEste projeto visa demonstrar a aplica√ß√£o pr√°tica do ChatGPT em um contexto espec√≠fico com o aux√≠lio do LangChain, explorando cada etapa e os principais desafios envolvidos."
  },
  {
    "objectID": "project1.html#setup-and-constants",
    "href": "project1.html#setup-and-constants",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Setup and Constants:",
    "text": "Setup and Constants:\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n# Setup\nfrom langchain.text_splitter             import RecursiveCharacterTextSplitter # Split - Tokenization\nfrom langchain.embeddings                import OpenAIEmbeddings               # Embeddings\nfrom langchain.vectorstores              import Chroma                         # Vector Store\nfrom langchain.llms                      import OpenAI                         # Models\nfrom langchain.chat_models               import ChatOpenAI                     # Chat\nfrom langchain.chains.question_answering import load_qa_chain                  # QA\nfrom langchain.callbacks                 import get_openai_callback            # Callback\nimport os\nimport textract\nimport warnings\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None) \n\nDefinindo as constantes que ser√£o utilizadas ao longo do c√≥digo.\n\n# Constants\nOPENAI_API_KEY       = os.environ.get('OPENAI_API_KEY')        # Definida como vari√°vel de ambiente\nFILE_PATH_CONTEXT    = \"./source/project1/context/cvPedro.pdf\" # Path para o arquivo de contexto\nEMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"                # Modelo para o embedding do contexto\nFILE_PATH_DB         = \"./source/project1/chroma/\"             # Path para Vector Store\nMODEL_NAME           = \"gpt-3.5-turbo\"                         # Modelo para responder as perguntas"
  },
  {
    "objectID": "project1.html#document-loading",
    "href": "project1.html#document-loading",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Document Loading",
    "text": "Document Loading\nAqui carregamos o arquivo de contexto, que pode estar em qualquer formato (https://textract.readthedocs.io/en/stable/) e convertemos em uma string.\n\n# Document Loading\nfile_path = FILE_PATH_CONTEXT\ndoc       = textract.process(file_path)\ntext      = doc.decode('utf-8')"
  },
  {
    "objectID": "project1.html#document-splitting---tokenization",
    "href": "project1.html#document-splitting---tokenization",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Document Splitting - Tokenization",
    "text": "Document Splitting - Tokenization\nNesta etapa, nosso objetivo √© segmentar o contexto em v√°rios documentos, com a principal ideia de dividir o texto em unidades semanticamente relevantes.\nPor simplicidade, essa divis√£o ser√° feita considerando um n√∫mero m√°ximo de caracteres por documento, levando em conta a estrutura textual original, ou seja, espa√ßos em branco, quebras de linhas e par√°grafos podem ser considerados como separadores aqui.\nExistem diversas t√©cnicas para efetuar este processo, estamos usando apenas uma delas.\n\n# Document Splitting - Tokenization\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 512, # Quantidade m√°xima de caracteres por split\n    chunk_overlap   = 24,  # Quantidade de m√°xima de caracteres sobrepostos por split\n)\n\nchunks = text_splitter.create_documents([text])"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "",
    "text": "O problema de churn, ou rotatividade de clientes, representa um desafio significativo para institui√ß√µes banc√°rias. Refere-se √† taxa na qual os clientes encerram seus relacionamentos com o banco, migrando para outras institui√ß√µes financeiras ou at√© mesmo abandonando servi√ßos financeiros. Este fen√¥meno pode ser impulsionado por diversos fatores, como insatisfa√ß√£o com servi√ßos, concorr√™ncia acirrada, mudan√ßas nas condi√ß√µes econ√¥micas e at√© mesmo avan√ßos tecnol√≥gicos.\n\n\nDessa forma, a reten√ß√£o de clientes torna-se uma prioridade estrat√©gica para os bancos, que buscam constantemente inovar, oferecer experi√™ncias mais atrativas e personalizadas, a fim de mitigar o churn e manter uma base s√≥lida de clientes leais.\n\n\nDiante desse cen√°rio, torna-se de extrema valia a capacidade de identificar os clientes mais propensos ao churn, possibilitando que a institui√ß√£o financeira aja de maneira antecipada, oferecendo planos e servi√ßos atrativos que impe√ßam a fuga do cliente.\n\n\nNeste projetos, temos como objetivo demonstrar a constru√ß√£o de modelo de ML para Identificar os clientes mais propensos ao churn no contexto banc√°rio."
  },
  {
    "objectID": "project2.html#an√°lise-explorat√≥ria-dos-dados-eda",
    "href": "project2.html#an√°lise-explorat√≥ria-dos-dados-eda",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "An√°lise Explorat√≥ria dos Dados (EDA)",
    "text": "An√°lise Explorat√≥ria dos Dados (EDA)\n\nO objetivo desta etapa √© investigar e entender os dados dispon√≠veis, a fim de extrair informa√ß√µes preliminares, identificar padr√µes, tend√™ncias, anomalias e insights relevantes.\n\n\nVerificaremos que, ao t√©rmino dessa an√°lise, todas as vari√°veis apresentam comportamento adequado para prosseguirem no estudo, pois:\n\nN√£o possuem valores faltantes.\nN√£o t√™m valores inesperados.\nN√£o exibem alta concentra√ß√£o em um √∫nico valor.\n\n\n\n\nChurn\nEssa vari√°vel denota a ocorr√™ncia ou n√£o do evento de churn.\n\nN√£o ocorr√™ncia de churn = 0\nOcorr√™ncia de churn = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel target (bin√°ria).\n\n\nTemos 20% de ocorr√™ncia do evento.\n\n\nTemos 80% de n√£o ocorr√™ncia do evento.\n\n\n\ncreate_table_categorical(\"churn\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n7963\n7963\n0.80\n0.80\n\n\n\n1\n2037\n10000\n0.20\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"churn\", df)\n\n\n\n\n\n\n\n\n\n\n\nCountry\nPa√≠s do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nFran√ßa representa 50%.\n\n\nAlemanha e Espanha representam 25% cada.\n\n\n\ncreate_table_categorical(\"country\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\nFrance\n5014\n5014\n0.50\n0.50\n\n\n\nGermany\n2509\n7523\n0.25\n0.75\n\n\n\nSpain\n2477\n10000\n0.25\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"country\", df)\n\n\n\n\n\n\n\n\n\n\n\nGender\nG√™nero do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n55% Homens.\n\n\n45% Mulheres.\n\n\n\ncreate_table_categorical(\"gender\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\nFemale\n4543\n4543\n0.45\n0.45\n\n\n\nMale\n5457\n10000\n0.55\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"gender\", df)\n\n\n\n\n\n\n\n\n\n\n\nCredit Card\nSe o cliente utiliza cart√£o de cr√©dito.\n\nN√£o utiliza cart√£o de cr√©dito = 0\nUtiliza cart√£o de cr√©dito = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n71% utiliza cart√£o de c≈ïedito.\n\n\n29% n√£o utiliza.\n\n\n\ncreate_table_categorical(\"credit_card\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n2945\n2945\n0.29\n0.29\n\n\n\n1\n7055\n10000\n0.71\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"credit_card\", df)\n\n\n\n\n\n\n\n\n\n\n\nActive Member\nDenota se um cliente est√° envolvido e participando ativamente das atividades e servi√ßos oferecidos pelo banco.\n\nN√£o ativo = 0\nAtivo = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPraticamente metade dos clientes s√£o ativos e a outra metade √© inativo.\n\n\n\ncreate_table_categorical(\"active_member\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n4849\n4849\n0.48\n0.48\n\n\n\n1\n5151\n10000\n0.52\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"active_member\", df)\n\n\n\n\n\n\n\n\n\n\n\nProducts Number\nN√∫mero de produtos adiquiridos pelo cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica discreta, mas ser√° visualizada como categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n~50% dos cliente utilizam somente um produto.\n\n\n~46% dos cliente utilizam 2 produtos.\n\n\n\ncreate_table_categorical(\"products_number\", df, ['1', '2', '3', '&gt;=4'])\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n1\n5084\n5084\n0.51\n0.51\n\n\n\n2\n4590\n9674\n0.46\n0.97\n\n\n\n3\n266\n9940\n0.03\n0.99\n\n\n\n&gt;=4\n60\n10000\n0.01\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"products_number\", df, ['1', '2', '3', '&gt;=4'])\n\n\n\n\n\n\n\n\n\n\n\nTenure\nMensura a quantidade de anos decorridos desde que o cliente aderiu aos servi√ßos prestados pelo banco.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica discreta, mas ser√° visualizada como categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n25% de 0 a 2 anos.\n\n\n15% de 8 a 10 anos.\n\n\nAs demais classes est√£o bem distribu√≠dos em torno de 20%.\n\n\n\n#fazer a mesma orden√ß√£o de baixo aqui\ncreate_table_categorical(\"tenure\", df, ['[0, 2]', '(2, 4]', '(4, 6]', '(6, 8]', '(8, 10]'])\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n[0, 2]\n2496\n2496\n0.25\n0.25\n\n\n\n(2, 4]\n1998\n4494\n0.20\n0.45\n\n\n\n(4, 6]\n1979\n6473\n0.20\n0.65\n\n\n\n(6, 8]\n2053\n8526\n0.21\n0.85\n\n\n\n(8, 10]\n1474\n10000\n0.15\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"tenure\", df, ['[0, 2]', '(2, 4]', '(4, 6]', '(6, 8]', '(8, 10]'])\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\n√â uma medida da probabilidade de um indiv√≠duo pagar suas d√≠vidas com base em seu hist√≥rico de cr√©dito passado.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPossui distribui√ß√£o aproximadamente normal, com m√©dia = 650.\n\n\n\ncreate_table_numeric_continuous(\"credit_score\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n6,505,288.00\n10000\n650.53\n96.65\n0.00\n0.00\n350.00\n432.00\n584.00\n652.00\n718.00\n778.00\n812.00\n850.00\n850.00\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"credit_score\", df)\n\n\n\n\n\n\n\n\n\n\n\nAge\nIdade em anos do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPossui distribui√ß√£o assim√©trica √° direita.\n\n\nCliente mais novo tem 18 anos.\n\n\nCliente mais velho tem 92 anos.\n\n\nO valor mediano da idade do cliente √© de 37 anos.\n\n\n\ncreate_table_numeric_continuous(\"age\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n389,218.00\n10000\n38.92\n10.49\n0.00\n0.00\n18.00\n21.00\n32.00\n37.00\n44.00\n53.00\n60.00\n72.00\n92.00\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"age\", df)\n\n\n\n\n\n\n\n\n\n\n\nBalance\nSaldo banc√°rio do cliente\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nVari√°vel com distribui√ß√£o assim√©trica.\n\n\nAlta concetra√ß√£o de clientes com saldo entre 0 e 25 mil.\n\n\n\ncreate_table_numeric_continuous(\"balance\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n764,858,892.88\n10000\n76,485.89\n62,397.41\n0.36\n0.00\n0.00\n0.00\n0.00\n97,198.54\n127,644.24\n149,244.79\n162,711.67\n185,967.99\n250,898.09\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"balance\", df)\n\n\n\n\n\n\n\n\n\n\n\nEstimated Salary\nSalario estimado.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nVari√°vel com distribui√ß√£o uniforme.\n\n\n\ncreate_table_numeric_continuous(\"estimated_salary\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n1,000,902,398.81\n10000\n100,090.24\n57,510.49\n0.00\n0.00\n11.58\n1,842.83\n51,002.11\n100,193.92\n149,388.25\n179,674.70\n190,155.38\n198,069.73\n199,992.48\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"estimated_salary\", df)"
  },
  {
    "objectID": "project2.html#an√°lise-bivariada-dos-dados",
    "href": "project2.html#an√°lise-bivariada-dos-dados",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "An√°lise Bivariada dos Dados",
    "text": "An√°lise Bivariada dos Dados\n\nA an√°lise bivariada tem como objetivo examinar a rela√ß√£o entre duas vari√°veis em um conjunto de dados. Ao contr√°rio da an√°lise univariada, que se concentra em uma √∫nica vari√°vel (EDA que fizemos no item anterior), a an√°lise bivariada explora a associa√ß√£o entre duas vari√°veis.\n\n\nExistem diferentes t√©cnicas e m√©todos para realizar uma an√°lise bivariada, dependendo da natureza das vari√°veis envolvidas. Para o nosso problema, estamos interessados em avaliar o grau de associa√ß√£o entre cada uma das poss√≠veis vari√°veis preditoras e a vari√°vel target, dessa forma temos que:\n\nchurn: Categ√≥rica Bin√°ria (target)\ncountry: Categ√≥rica\ngender: Categ√≥rica\ncredit_card: Categ√≥rica\nactive_member: Categ√≥rica\nproducs_number: Num√©rica Discreta\ntenure: Num√©rica Discreta\ncredit_score: Num√©rica Cont√≠nua\nage: Num√©rica Discreta\nbalance: Num√©rica Cont√≠nua\nestimated_salary: Num√©rica Cont√≠nua\n\n\n\nA nossa abordagem ser√° transformar todas as vari√°veis preditoras em categ√≥ricas, para posteriormente avaliarmos o grau de associa√ß√£o de cada uma delas frente a vari√°vel target, para isso teremos basicamente duas etapas:\n\n\nBinning das vari√°veis.\n\n\nAvalia√ß√£o do Grau de Associa√ß√£o.\n\n\n\n\nBinning\n\nChamaremos esse processo de transforma√ß√£o de uma vari√°vel num√©rica em categ√≥rica de binning. O processo de binning ser√° feito atrav√©s de um m√©todo denominado de optimal binning https://gnpalencia.org/optbinning/.\n\n\nO optimal binning refere-se a uma abordagem estat√≠stica utilizada em an√°lise de dados para agrupar valores de uma vari√°vel em intervalos (ou ‚Äúbins‚Äù) de maneira a otimizar algum crit√©rio espec√≠fico. A principal ideia por tr√°s do binning √≥timo √© encontrar a divis√£o mais informativa ou significativa das observa√ß√µes, geralmente com base em algum crit√©rio de interesse, como a maximiza√ß√£o da diferen√ßa nas m√©dias entre os grupos ou a minimiza√ß√£o da variabilidade intra-bin.\n\n\nOu seja, de forma resumida, vamos pegar um certa vari√°vel, por exemplo o balance (saldo da conta) e tentar discretizar em categorias onde fique mais evidente se a ocorr√™ncia de churn √© maior ou menor.\n\n\nVale ressaltar que tamb√©m vamos aplicar o optimal binning para as vari√°veis que j√° s√£o categ√≥ricas, uma vez que esse processo de otimiza√ß√£o pode gerar agrupamentos mais informativos (quanto a ocorr√™ncia de churn) do que as categorias j√° existentes.\n\n\nAvalia√ß√£o do Grau de Associa√ß√£o\n\nNeste momento todas as nossas vari√°veis (preditoras e target) s√£o categ√≥ricas, ent√£o para mensurar o grau de associa√ß√£o entre cada preditora e o target, utilizaremos o coeficiente Cramer‚Äôs V (V de Cramer).\n\n\nO coeficiente V de Cramer √© uma medida estat√≠stica utilizada em an√°lises bivariadas para quantificar a for√ßa de associa√ß√£o entre duas vari√°veis categ√≥ricas. Essa medida √© uma extens√£o do coeficiente qui-quadrado, que √© comumente utilizado para testar a independ√™ncia entre vari√°veis categ√≥ricas.\nO coeficiente V de Cramer varia de 0 a 1, onde 0 indica nenhuma associa√ß√£o e 1 indica associa√ß√£o total entre as vari√°veis categ√≥ricas. https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n\n\nResultado da An√°lise Bivariada\nAp√≥s o t√©rmino da an√°lise bivariada, conforme a tabela abaixo e os demais resultados que veremos na sequ√™ncia, veremos que:\n\nage: √â a vari√°vel com maior grau de associa√ß√£o com o evento de churn (Alta discrimin√¢ncia).\nproducts_number, country, active_member: Est√£o associadas de forma moderada com o evento de churn (M√©dia discrimin√¢ncia).\ncredit_score, tenure, estimated_salary e credit_card: Possuem baixo grau de associa√ß√£o com o evento de churn (Baixa discrimin√¢ncia).\n\nAs vari√°veis com alta e m√©dia discrimin√¢ncia t√™m maiores chances de serem consideradas como preditoras no modelo preditivo que iremos construir. Em contrapartida, as vari√°veis de baixa discrimin√¢ncia possuem menor propens√£o de serem utilizadas como preditoras nesse modelo.‚Äù\nObserva√ß√£o\n\nO crit√©rio utilizado para definir a discrimin√¢ncia n√£o est√° levando em conta apenas se o valor de Cramer‚Äôs V est√° muito pr√≥ximo de 0 ou 1, mas tamb√©m considera o contexto dessa an√°lise. Por exemplo, para a vari√°vel idade, temos Cramer‚Äôs V = 0.36, que √© um valor mais pr√≥ximo de 0 do que de 1. Se consider√°ssemos apenas essa quest√£o, dir√≠amos que o grau de associa√ß√£o √© moderado ou baixo.\nNo entanto, no contexto desta an√°lise, a vari√°vel idade √© a que possui o maior Cramer‚Äôs V. Portanto, dentro do nosso contexto, estamos considerando que o grau de associa√ß√£o com o evento √© forte.\nPara fins de esclarecimento, consideramos que:\n\nCramer‚Äôs V &gt; 0.20: Discrimin√¢ncia Alta.\n0.20 &lt;= Cramer‚Äôs V &lt; 0.05: Discrimin√¢ncia M√©dia.\nCramer‚Äôs V &lt;= 0.05: Discrimin√¢ncia Baixa.\n\n\n\nnumerical_variables = [\n    \"products_number\",\n    \"tenure\",\n    \"credit_score\",\n    \"age\",\n    \"estimated_salary\"\n    ]\n\ncategorical_variables = [\n    \"country\", \n    \"gender\", \n    \"credit_card\",\n    \"active_member\",\n    \"balance\",\n    ]\n\ntarget_variable = 'churn'\n\ndf = df_base\n\n# balance\nvar_name = 'balance'\nc1 = df[var_name].between(-np.inf,  1884.34, inclusive = \"neither\")\ndf[var_name] = np.where(c1, '&lt; 1884.34', '&gt;= 1884.34')\n\ndf_bivariate = bivariate(\n    df,\n    numerical_variables,\n    categorical_variables,\n    target_variable)\n\ndf_bivariate.rename(columns = {\"Indicador\":\"Feature\"}, inplace = True)\n\ndf = (\n  df_bivariate[[\"Feature\", \"Cramer's V\", \"Discrimin√¢ncia\"]]\n  .drop_duplicates()\n  .sort_values(by = [\"Cramer's V\", \"Feature\"], ascending = False)\n  .reset_index(drop = True)\n  )\n    \ncreate_table_bivariate_summary(df, cols_float = [\"Cramer's V\"])\n\n\n\n\n\n\n¬†\nFeature\nCramer's V\nDiscrimin√¢ncia\n\n\n\n\n\nage\n0.36\nAlta\n\n\n\nproducts_number\n0.19\nM√©dia\n\n\n\ncountry\n0.17\nM√©dia\n\n\n\nactive_member\n0.16\nM√©dia\n\n\n\nbalance\n0.12\nM√©dia\n\n\n\ngender\n0.11\nM√©dia\n\n\n\ncredit_score\n0.04\nBaixa\n\n\n\ntenure\n0.04\nBaixa\n\n\n\nestimated_salary\n0.03\nBaixa\n\n\n\ncredit_card\n0.01\nBaixa\n\n\n\n\n\n\nAge\nDiscrimin√¢ncia:  Alta. \n\n\nQuanto mais velho for o cliente, maior √© a propens√£o de ocorr√™ncia do churn.\n\n\n\nvar = \"age\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 27.50)\n947\n73\n1020\n7.16\n10.20\n\n\n\n[27.50, 32.50)\n1630\n140\n1770\n7.91\n17.70\n\n\n\n[32.50, 34.50)\n812\n77\n889\n8.66\n8.89\n\n\n\n[34.50, 36.50)\n820\n110\n930\n11.83\n9.30\n\n\n\n[36.50, 38.50)\n830\n125\n955\n13.09\n9.55\n\n\n\n[38.50, 40.50)\n694\n161\n855\n18.83\n8.55\n\n\n\n[40.50, 42.50)\n541\n146\n687\n21.25\n6.87\n\n\n\n[42.50, 46.50)\n670\n339\n1009\n33.60\n10.09\n\n\n\n[46.50, inf)\n1019\n866\n1885\n45.94\n18.85\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nProducts Number\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes que contrataram somente 1 produto.\n\n\n\nvar = \"products_number\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 1.50)\n3675\n1409\n5084\n27.71\n50.84\n\n\n\n[1.50, inf)\n4288\n628\n4916\n12.77\n49.16\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCountry\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes da Alemanha.\n\n\n\nvar = \"country\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['France']\n4204\n810\n5014\n16.15\n50.14\n\n\n\n['Germany']\n1695\n814\n2509\n32.44\n25.09\n\n\n\n['Spain']\n2064\n413\n2477\n16.67\n24.77\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nActive Member\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes que n√£o s√£o ativos.\n\n\n\nvar = \"active_member\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['0']\n3547\n1302\n4849\n26.85\n48.49\n\n\n\n['1']\n4416\n735\n5151\n14.27\n51.51\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nBalance\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes com saldo em conta maior ou igual a 1884.34 .\n\n\n\nvar = \"balance\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['&lt; 1884.34']\n3117\n500\n3617\n13.82\n36.17\n\n\n\n['&gt;= 1884.34']\n4846\n1537\n6383\n24.08\n63.83\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nGender\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes do sexo feminino.\n\n\n\nvar = \"gender\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['Female']\n3404\n1139\n4543\n25.07\n45.43\n\n\n\n['Male']\n4559\n898\n5457\n16.46\n54.57\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado de qu√£o bom ou ruim √© o credit_score do cliente, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"credit_score\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 489.50)\n377\n125\n502\n24.90\n5.02\n\n\n\n[489.50, 552.50)\n903\n254\n1157\n21.95\n11.57\n\n\n\n[552.50, 629.50)\n1937\n514\n2451\n20.97\n24.51\n\n\n\n[629.50, 651.50)\n696\n181\n877\n20.64\n8.77\n\n\n\n[651.50, 678.50)\n859\n194\n1053\n18.42\n10.53\n\n\n\n[678.50, 703.50)\n782\n162\n944\n17.16\n9.44\n\n\n\n[703.50, 734.50)\n816\n195\n1011\n19.29\n10.11\n\n\n\n[734.50, inf)\n1593\n412\n2005\n20.55\n20.05\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nTenure\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado da quantidade de anos decorridos desde que o cliente aderiu aos servi√ßos prestados pelo banco, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"tenure\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 1.50)\n1121\n327\n1448\n22.58\n14.48\n\n\n\n[1.50, 5.50)\n3232\n826\n4058\n20.35\n40.58\n\n\n\n[5.50, 6.50)\n771\n196\n967\n20.27\n9.67\n\n\n\n[6.50, 7.50)\n851\n177\n1028\n17.22\n10.28\n\n\n\n[7.50, 8.50)\n828\n197\n1025\n19.22\n10.25\n\n\n\n[8.50, inf)\n1160\n314\n1474\n21.30\n14.74\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nEstimated Salary\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado de qu√£o alto ou baixo √© o sal√°rio do cliente, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"estimated_salary\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 33482.46)\n1297\n343\n1640\n20.91\n16.40\n\n\n\n[106162.09, 169420.42)\n2474\n647\n3121\n20.73\n31.21\n\n\n\n[169420.42, inf)\n1201\n341\n1542\n22.11\n15.42\n\n\n\n[33482.46, 73970.20)\n1620\n402\n2022\n19.88\n20.22\n\n\n\n[73970.20, 83463.54)\n423\n78\n501\n15.57\n5.01\n\n\n\n[83463.54, 106162.09)\n948\n226\n1174\n19.25\n11.74\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCredit Card\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado do cliente ter ou n√£o cart√£o de c≈ïedito, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"credit_card\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['0']\n2332\n613\n2945\n20.81\n29.45\n\n\n\n['1']\n5631\n1424\n7055\n20.18\n70.55\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)"
  },
  {
    "objectID": "project2.html#o-modelo-preditivo",
    "href": "project2.html#o-modelo-preditivo",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "O Modelo Preditivo",
    "text": "O Modelo Preditivo\nEm desenvolvimento - N√£o est√° pronto ainda\nT√©cnica Utilizada\n\nQuanto √† modelagem, um ponto deve ser enfatizado: o principal objetivo aqui √© a constru√ß√£o de um modelo preditivo que possua um grau de interpretabilidade relativamente f√°cil para a √°rea de neg√≥cios. Portanto, nosso foco √© obter n√£o apenas um modelo assertivo, mas sim um modelo com bom desempenho e de f√°cil entendimento. Dito isso, vamos modelar utilizando a t√©cnica de Regress√£o Log√≠stica.\n\n\nA regress√£o log√≠stica √© uma t√©cnica estat√≠stica utilizada para modelar a rela√ß√£o entre uma vari√°vel dependente bin√°ria (que possui apenas dois valores poss√≠veis, geralmente 0 e 1) e uma ou mais vari√°veis independentes. Ela √© amplamente empregada em problemas de classifica√ß√£o, onde o objetivo √© prever a probabilidade de uma observa√ß√£o pertencer a uma determinada categoria.\n\n\nA principal caracter√≠stica da regress√£o log√≠stica √© sua capacidade de lidar com problemas de classifica√ß√£o bin√°ria, como por exemplo, prever se um e-mail √© spam ou n√£o, se um paciente tem uma determinada condi√ß√£o m√©dica ou n√£o, entre outros cen√°rios onde a resposta desejada √© dicot√¥mica.\n\n\nA forma b√°sica da regress√£o log√≠stica √© expressa pela seguinte equa√ß√£o:\n\n\n\n\n\nContextualizando a formula acima para o nosso problema, temos que:\n\nX1, X2, ‚Ä¶ Xn s√£o as nossas features: country, gender, credit_card, active_member, producs_number, tenure, credit_score, age, balance, estimated_salary.\nA probabilide p do evento Y = 1, √© a probabilidade de ocorr√™ncia de churn.\nOs betas B0, B1, ‚Ä¶ Bn representam os par√¢metros do modelo, que pretendemos obter ao construir o modelo matem√°tico.\n\n\n\nAvalia√ß√£o do Modelo\n\nO teste de Kolmogorov-Smirnov (KS) pode ser utilizado como uma m√©trica para avaliar a qualidade de modelos de classifica√ß√£o, especialmente em problemas de classifica√ß√£o bin√°ria. Nesse contexto, o KS √© frequentemente empregado para avaliar a capacidade do modelo em distinguir entre as classes positiva e negativa.\n\n\nA abordagem mais comum envolve a gera√ß√£o de pontua√ß√µes (scores) ou probabilidades de predi√ß√£o para as inst√¢ncias de ambas as classes pelo modelo. Em seguida, o teste de Kolmogorov-Smirnov √© aplicado √†s distribui√ß√µes cumulativas dessas pontua√ß√µes para as duas classes. O objetivo √© verificar se h√° uma diferen√ßa significativa entre as distribui√ß√µes cumulativas das classes positiva e negativa.\n\n\nQuanto maior for a diferen√ßa entre as distribui√ß√µes cumulativas, maior ser√° o valor de KS e, consequentemente, mais eficiente ser√° o modelo, ou seja, mais capaz de classificar corretamente o que √© churn e o que n√£o √© churn.\n\n\nObtendo o melhor modelo\n\nA partir das 10 vari√°veis preditoras dispon√≠veis, foram testadas todas as combina√ß√µes poss√≠veis, selecionando de 5 a 10 vari√°veis por vez. Dessa forma, examinamos 638 modelos de regress√£o log√≠stica, avaliando o valor de KS para cada modelo nos dados de teste. Optamos pelo modelo que apresentou o maior valor de KS e o menor n√∫mero de vari√°veis preditoras que parecem ser mais relevantes para o neg√≥cio.\n\n\nPortanto, o modelo selecionado foi o que apresentou KS = 0.36 e possui as seguintes vari√°veis preditoras:\n\n‚Äòproducts_number‚Äô\n‚Äòcountry‚Äô\n‚Äògender‚Äô\n‚Äòcredit_card‚Äô\n‚Äòactive_member‚Äô\n\n\n\nRepresenta√ß√£o gr√°fica do c√°lculo do KS nos dados de teste para o modelo selecionado.\n\n\nseed          = 100\nX             = df[['products_number', 'country', 'gender', 'credit_card', 'active_member']].copy()\ny             = df[target_variable].copy()\noriginal_cols = X.columns\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n\n# ----------------------------------- Binnings --------------------------------\ndf_train = pd.merge(X_train,\n                    y_train, \n                    how         = \"left\", \n                    left_index  = True, \n                    right_index = True,\n                    validate    = \"one_to_one\")\n\ndf_test = pd.merge(X_test,\n                   y_test, \n                   how         = \"left\", \n                   left_index  = True, \n                   right_index = True,\n                   validate    = \"one_to_one\")\n            \n\ndf_train = binning_to_model(\n    df_train,\n    list(df_train.columns[df_train.columns.isin(numerical_variables)]),\n    list(df_train.columns[df_train.columns.isin(categorical_variables)]),\n    target_variable).reset_index(drop = True)\n\ndf_test = binning_to_model(\n    df_test,\n    list(df_test.columns[df_test.columns.isin(numerical_variables)]),\n    list(df_test.columns[df_test.columns.isin(categorical_variables)]),\n    target_variable).reset_index(drop = True)\n\nX_train  = df_train.drop(target_variable, axis=1).copy()\ny_train  = df_train[target_variable].copy()\n\nX_test  = df_test.drop(target_variable, axis=1).copy()\ny_test  = df_test[target_variable].copy()\n \n# -----------------------------------------------------------------------------\n\n# One-hot Encoding\nenc = OneHotEncoder(handle_unknown='ignore', drop = 'first')\nenc.fit(X_train.astype(str))\n\ncolnames = enc.get_feature_names_out()\n\n# train\ntransformed = enc.transform(X_train.astype(str)).toarray()\ndf_cat_vars = pd.DataFrame(columns=colnames, data=transformed)\nX_train = pd.concat([X_train, df_cat_vars], axis=1)\n\n# test\ntransformed = enc.transform(X_test.astype(str)).toarray()\ndf_cat_vars = pd.DataFrame(columns=colnames, data=transformed)\nX_test = pd.concat([X_test, df_cat_vars], axis=1)\n\n# Remocao das vari√°veis categoricas sem codifica√ß√£o\nX_train.drop(original_cols, axis=1, inplace = True)\nX_test.drop(original_cols, axis=1, inplace = True)\n\n# Treino\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n#nos_train = create_ks_table_for_logistic_regression(clf, X_train, y_train)\nnos_test  = create_ks_table_for_logistic_regression(clf, X_test, y_test)\n\n\ny_pred = np.round(clf.predict_proba(X_test)[:, 1], 2)\ny_pred = np.where(y_pred &gt;= 0.19, 1, 0)\n \ny_test = np.where(y_test == 1, \"Churn\", \"N√£o-Churn\")\ny_pred = np.where(y_pred == 1, \"Churn\", \"N√£o-Churn\")\n\n\n\n\n\n\n\n\nFaixas de opera√ß√£o do modelo selecionado\n\nA tabela abaixo apresenta todas as poss√≠veis faixas de opera√ß√£o do modelo selecionado a qual foi aplicada na base de testes. Do ponto de vista t√©cnico, a faixa 8 em destaque √© considerada a faixa √≥tima, pois possui Sens. + Espec. - 1 = KS = 0. Isso significa que essa √© a faixa com maior capacidade de classifica√ß√£o correta do que √© Churn e N√£o-Churn.\n\n\nInterpretando a tabela para essa faixa, temos que:\nProb = 19%: Significa que, se a probabilidade de ocorr√™ncia do churn for maior ou igual a 19%, o modelo classificar√° como churn.\n% Total acumulado = 40%: Significa que, dos clientes que o algoritmo avaliar, ele classificar√° 40% como churn.\nTx. classificados como 1 corretamente = 35%: Significa que, dos clientes que o algoritmo classificar como churn, 35% realmente realizariam churn, enquanto os outros 65% n√£o realizariam.\nTx. classificados como 0 corretamente = 89%: Significa que, dos clientes que o algoritmo classificar como N√£o-Churn, 89% realmente n√£o realizariam churn, enquanto os outros 11% realizariam.\nSens. = 68%: √â a sensibilidade. Significa que, de todos os casos de churn, o modelo ser√° capaz de classificar corretamente 68% deles, enquanto os outros 32% ser√£o classificados como N√£o-Churn.\nEspec. = 67%: √â a especificidade. Significa que, de todos os casos de N√£o-churn, o modelo ser√° capaz de classificar corretamente 67% deles, enquanto os outros 33% ser√£o classificados como Churn.\nSens. + Espec. - 1 = 0.36: Utilizado para calcular o valor aproximado de KS e comparar com o KS obtido no gr√°fico acima.\nAcur√°cia: De todos os clientes avaliados pelo algoritmo, o percentual de acertos, incluindo churn e n√£o churn, √© de 67%.\n\n\nnos_test.drop([\"Evento acumulado\", \"Nao-evento acumulado\", \"Ganho 1's\",\"Ganho 0's\"], axis = 1, inplace = True)\n\nCOLOR           = \"#001820\"\ndf              = nos_test\ncols_to_percent = [\n  \"Prob\",\n  \"% Total acumulado\",\n  \"Tx. classificados como 1 corretamente\",\n  \"Tx. classificados como 0 corretamente\", \n  \"Sens.\", \n  \"Espec.\",\n  \"Acur√°cia\", \n  ]\n\ncols_to_float = [\"Sens. + Espec. - 1\"]\n\ndf_styled = (\n    df.style\n        # Cor do header e index\n        .set_table_styles([{\n            'selector': 'th:not(.index_name)',\n            'props': f'background-color: {COLOR}; color: white; text-align: center;'\n        }]) \n        .set_properties(\n          **{'text-align': \"center\"})\n        .set_properties(\n            subset = ([7],) ,\n            **{'background-color': \"#C5D9F1\",\n                'color'          : \"black\",\n                'font-weight'    : \"bold\",\n                'text-align'     : \"center\"\n              })\n        .format('{:.0%}', subset= cols_to_percent) \n        .format('{:.2f}', subset= cols_to_float) \n        .hide(axis=\"index\")\n    )\ndf_styled\n\n\n\n\n\n\nFaixa\nProb\n% Total acumulado\nTx. classificados como 1 corretamente\nTx. classificados como 0 corretamente\nSens.\nEspec.\nSens. + Espec. - 1\nAcur√°cia\n\n\n\n\n1\n44%\n5%\n49%\n81%\n12%\n97%\n0.09\n79%\n\n\n2\n38%\n10%\n47%\n82%\n23%\n93%\n0.16\n79%\n\n\n3\n34%\n16%\n42%\n84%\n34%\n88%\n0.22\n77%\n\n\n4\n34%\n20%\n43%\n85%\n43%\n86%\n0.28\n77%\n\n\n5\n26%\n26%\n39%\n86%\n51%\n80%\n0.31\n74%\n\n\n6\n23%\n31%\n37%\n87%\n56%\n75%\n0.31\n71%\n\n\n7\n23%\n36%\n36%\n88%\n63%\n72%\n0.34\n70%\n\n\n8\n19%\n40%\n35%\n89%\n68%\n67%\n0.36\n67%\n\n\n9\n19%\n47%\n33%\n90%\n75%\n60%\n0.35\n63%\n\n\n10\n17%\n51%\n31%\n90%\n77%\n56%\n0.33\n60%\n\n\n11\n14%\n56%\n30%\n91%\n81%\n51%\n0.31\n57%\n\n\n12\n13%\n60%\n28%\n91%\n82%\n46%\n0.28\n53%\n\n\n13\n12%\n65%\n27%\n91%\n84%\n40%\n0.24\n49%\n\n\n14\n12%\n70%\n25%\n91%\n87%\n34%\n0.21\n45%\n\n\n15\n10%\n80%\n23%\n90%\n90%\n23%\n0.13\n37%\n\n\n16\n10%\n81%\n23%\n91%\n91%\n22%\n0.13\n36%\n\n\n17\n8%\n89%\n22%\n92%\n96%\n13%\n0.08\n30%\n\n\n18\n6%\n90%\n22%\n92%\n96%\n11%\n0.07\n29%\n\n\n19\n5%\n100%\n21%\n0%\n100%\n0%\n0.00\n21%\n\n\n\n\n\nConclus√µes Sobre o Modelo\nMas afinal, esse modelo √© bom ou ruim? Para responder a essa pergunta, devemos comparar duas situa√ß√µes: quando o modelo √© utilizado e quando n√£o √©.\nSem o modelo:\n\nSem o modelo, n√£o temos nenhuma outra forma de identificar quais clientes estariam mais propensos ao churn e tentar alguma abordagem para resgatar a confian√ßa deles, evitando sua fuga.\nIsso ocorrendo, o banco teria uma redu√ß√£o de cerca de 20% de seus clientes.\n\nCom o modelo:\n\n68% dos casos poss√≠veis de churn seriam identificados previamente, enquanto os demais 32% n√£o seriam identificados.\nSupondo que um plano de a√ß√£o seja capaz de impedir o churn de todos os poss√≠veis churn identificados (os 68%), o banco teria uma redu√ß√£o em sua carteira de clientes de apenas 6,4% (32% dos churns, os quais n√£o foram identificados pelo modelo).\nObviamente, os erros do modelo em classificar N√£o-churn como churn tamb√©m geram custos para o banco, pois ele gastar√° dinheiro para resgatar um cliente que, de fato, n√£o corria risco de perda. Isso ocorrer√° em 65% dos casos em que o modelo classificar como churn. De fato, 40% de todos os clientes ser√£o classificados como churn pelo modelo, sendo que apenas 35% desse p√∫blico realizaria o churn. A quest√£o √© que o modelo adota uma postura muito conservadora, onde a perda do cliente √© considerada algo muito mais severo do que o gasto com programas para evitar o churn. Por isso, se a probabilidade de ocorr√™ncia do churn for maior ou superior a 19%, o modelo prefere recomendar uma a√ß√£o do banco para tentar evitar a fuga desse cliente.\nDiante do exposto anteriormente, podemos afirmar que o modelo tem uma boa performance. No entanto, a decis√£o final de utiliz√°-lo ou n√£o viria ap√≥s uma an√°lise de ganho financeiro, comparando o seu uso com a n√£o utiliza√ß√£o."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projetos",
    "section": "",
    "text": "Os projetos apresentados aqui ser√£o sempre aplicados em bases de dados p√∫blicas. No entanto, utilizarei a mesma abordagem profissional que emprego no meu dia a dia. O objetivo √© que, √† medida que eu criar novos projetos (ou avan√ßar dentro de um deles), eu v√° atualizando esta p√°gina. Portanto, constantemente haver√° atualiza√ß√µes aqui.\nEm caso de d√∫vidas e/ou sugest√µes, sinta-se √† vontade para me contatar via inbox no LinkedIn."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "",
    "text": "A busca por similaridade (Similarity Search) no contexto da Linguagem Natural (NLP) desempenha um papel crucial em diversas aplica√ß√µes, desde a recupera√ß√£o de informa√ß√µes at√© a an√°lise de sentimentos e a recomenda√ß√£o de conte√∫do. Essa abordagem consiste em encontrar itens semelhantes a um item de consulta, onde a semelhan√ßa √© avaliada atrav√©s da dist√¢ncia entre vetores num√©ricos que representam segmentos de texto (embeddings).\n\n\nNos modelos de linguagem de grande escala (LLM) no contexto de NLP, a busca por similaridade √© frequentemente empregada na engenharia de prompts, onde apenas os documentos mais relevantes para responder a uma determinada pergunta s√£o selecionados e passados para o modelo de LLM. Isso ajuda a otimizar o desempenho do modelo, reduzindo a carga computacional e melhorando a precis√£o das respostas fornecidas.\n\n\nPara facilitar a implementa√ß√£o da busca por similaridade, muitas vezes s√£o utilizados bancos de dados e ferramentas otimizadas para esse fim, como o MongoDB cluster. O MongoDB cluster oferece suporte para indexa√ß√£o de dados de texto e consultas avan√ßadas que permitem recuperar documentos com base em sua similaridade com uma consulta espec√≠fica. Essa capacidade √© fundamental para sistemas que lidam com grandes volumes de dados textuais e precisam fornecer respostas r√°pidas e precisas aos usu√°rios.\n\n\nAl√©m do MongoDB cluster, existe o Chroma que √© uma op√ß√£o interessante para realizar buscas por similaridade localmente. Como um banco de dados open source projetado especialmente para suportar opera√ß√µes de busca por similaridade em dados de alta dimensionalidade, como embeddings de texto, o Chroma oferece uma solu√ß√£o eficiente e escal√°vel para realizar tarefas de NLP localmente, sem a necessidade de configurar e gerenciar um ambiente distribu√≠do.\n\n\nDiante da import√¢ncia e relev√¢ncia da busca por similaridade no contexto da NLP, vamos construir um projeto para demonstrar como realizar essa tarefa utilizando o Chroma."
  },
  {
    "objectID": "project3.html#setup-and-constants",
    "href": "project3.html#setup-and-constants",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Setup and Constants",
    "text": "Setup and Constants\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n#%% Setup\nfrom chromadb.utils           import embedding_functions            # Embbeding\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter # Split\nfrom transformers             import GPT2TokenizerFast              # Tokenization\nimport chromadb                                                     # DB\nimport textract                                                     # load data\n\n/home/pedrods/Documents/repos/rest_api/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "project3.html#load-data",
    "href": "project3.html#load-data",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Load data",
    "text": "Load data\nAqui, vamos carregar um arquivo de texto arbitr√°rio. Neste exemplo, vamos utilizar o meu curr√≠culo, o mesmo utilizado no projeto https://pedrodubiela95.github.io/portfolio/project1.html.\n\n#%% Load files\nfile_path = \"./source/project1/context/cvPedro.pdf\"\ndoc       = textract.process(file_path)\ndata      = doc.decode('utf-8')"
  },
  {
    "objectID": "project3.html#document-splitting---tokenization",
    "href": "project3.html#document-splitting---tokenization",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Document Splitting - Tokenization",
    "text": "Document Splitting - Tokenization\nNesta etapa segmentamos nosso arquivo de texto em documentos.\n\n#%% Split into documents\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -&gt; int:\n    return len(tokenizer.encode(text))\n\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size    = 100, # Quantidade m√°xima de caracteres por split\n    chunk_overlap = 15,  # Quantidade de caracteres sobrepostos por split\n    length_function = count_tokens,\n)\n\nsplits = text_splitter.create_documents([data])"
  },
  {
    "objectID": "project3.html#embedding",
    "href": "project3.html#embedding",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Embedding",
    "text": "Embedding\nAplicamos o embedding atrav√©s do modelo open-source ‚Äúall-mpnet-base-v2‚Äù\n\n#%% Embbeding\nsentence_transformer_ef = (\n    embedding_functions\n    .SentenceTransformerEmbeddingFunction(\n        model_name=\"all-mpnet-base-v2\"\n        )\n    )"
  },
  {
    "objectID": "project3.html#create-db-and-collection",
    "href": "project3.html#create-db-and-collection",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Create DB and collection",
    "text": "Create DB and collection\nCriamos o banco e a collection, e informamos qual tipo de embedding ser√° utilizado, bem como o tipo de dist√¢ncia utilizado no c√°lculo da similaridade entre os vetores gerados (embeddings).\n\n#%% Local to save DB\nclient = chromadb.PersistentClient(path=\"./\")\nclient.heartbeat()\n\n# Try delete collection\ntry:\n    client.delete_collection(name=\"my_collection\")\nexcept:\n    pass\n\n# Create a collection\ncollection = client.create_collection(\n    name               =\"my_collection\",\n    metadata           = {\"hnsw:space\": \"cosine\"},\n    embedding_function = sentence_transformer_ef\n    )\n\nAdicionamos os documentos na collection.\n\n# Add data\ncollection.add(\n    documents = [s.page_content for s in splits],\n    ids       = [f\"id{i}\" for i in range(len(splits))]\n    )\n\n# Qty documents in collections\n#collection.count()\n#collection.peek()"
  },
  {
    "objectID": "project3.html#similarity-search",
    "href": "project3.html#similarity-search",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Similarity Search",
    "text": "Similarity Search\nEfetuamos de fato a busca por similaridade.\nExemplo1: Vamos fazer a seguinte pergunta Onde o Pedro mora? e retornar qual segmento de texto contido no curr√≠culo possui maior simililaridade com essa pergunta.\n\n#%% Similarity Search\n\nquery = \"Onde o Pedro mora?\"\nquery_vector = sentence_transformer_ef(query)\nres = collection.query(\n    query_embeddings= query_vector,\n    n_results = 1,\n    include=['distances','embeddings', 'documents', 'metadatas']\n    )\nfirst = res[\"documents\"][0][0]\nprint(first)\n\nPedro Gasparine Dubiela\nAlt√¥nia - PR\n\nhttps://pedrodubiela95.github.io/portfolio/\n\npedrodubielabio@gmail.com\n\n(44) 9-9708-9090\n\n\nExemplo2: Vamos fazer a seguinte pergunta Quais as habildiades do Pedro? e retornar qual segmento de texto contido no curr√≠culo possui maior simililaridade com essa pergunta.\n\nquery = \"Quais as habildiades do Pedro?\"\nquery_vector = sentence_transformer_ef(query)\nres = collection.query(\n    query_embeddings= query_vector,\n    n_results = 1,\n    include=['distances','embeddings', 'documents', 'metadatas']\n    )\nfirst = res[\"documents\"][0][0]\nprint(first)\n\nHabilidades\n&gt;&gt;&gt; Ingl√™s Intermedi√°rio (B1-B2)\n&gt;&gt;&gt; Forte background em matem√°tica e estat√≠stica\n&gt;&gt;&gt; S√≥lidos conhecimentos em ci√™ncia da computa√ß√£o\n&gt;&gt;&gt; An√°lise de dados"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "üë®üèª‚Äçüéì Bacharelado em Engenharia Qu√≠mica | Universidade Estadual de Maring√° - UEM | 2016-2020.\nüèÖ Especialista em Intelig√™ncia Artificial e Computacional | Universidade Federal de Vi√ßosa - UFV | 2023.\nüî≠ Atualmente, trabalho como Cientista de Dados, focado na constru√ß√£o de modelos preditivos para o agroneg√≥cio.\nüå± Busco aprimoramento constante de acordo com as melhores pr√°ticas de MLOps.\nüí¨ Podemos discutir sobre todo o ciclo de vida de modelos de Aprendizado de M√°quina.\nüòÑ Pronomes: Ele/Dele.\n‚ö° Curiosidade: Conclu√≠ minha gradua√ß√£o em Engenharia Qu√≠mica no final de 2020. Em seguida, iniciei minha jornada na ind√∫stria qu√≠mica no setor de P&D. No entanto, minha paix√£o por tecnologia e an√°lise de dados come√ßou a falar mais alto (uso programa√ß√£o de computadores desde 2016, resolvendo problemas de engenharia). Decidi, ent√£o, ingressar no mundo da Ci√™ncia de Dados, come√ßando na √°rea de BI/BA e, posteriormente, atuando como Cientista de Dados com foco no desenvolvimento e implementa√ß√£o de modelos de Aprendizado de M√°quina."
  },
  {
    "objectID": "index.html#ol√°-seja-bem-vindo-ao-meu-portf√≥lio.",
    "href": "index.html#ol√°-seja-bem-vindo-ao-meu-portf√≥lio.",
    "title": "Home",
    "section": "",
    "text": "üë®üèª‚Äçüéì Bacharelado em Engenharia Qu√≠mica | Universidade Estadual de Maring√° - UEM | 2016-2020.\nüèÖ Especialista em Intelig√™ncia Artificial e Computacional | Universidade Federal de Vi√ßosa - UFV | 2023.\nüî≠ Atualmente, trabalho como Cientista de Dados, focado na constru√ß√£o de modelos preditivos para o agroneg√≥cio.\nüå± Busco aprimoramento constante de acordo com as melhores pr√°ticas de MLOps.\nüí¨ Podemos discutir sobre todo o ciclo de vida de modelos de Aprendizado de M√°quina.\nüòÑ Pronomes: Ele/Dele.\n‚ö° Curiosidade: Conclu√≠ minha gradua√ß√£o em Engenharia Qu√≠mica no final de 2020. Em seguida, iniciei minha jornada na ind√∫stria qu√≠mica no setor de P&D. No entanto, minha paix√£o por tecnologia e an√°lise de dados come√ßou a falar mais alto (uso programa√ß√£o de computadores desde 2016, resolvendo problemas de engenharia). Decidi, ent√£o, ingressar no mundo da Ci√™ncia de Dados, come√ßando na √°rea de BI/BA e, posteriormente, atuando como Cientista de Dados com foco no desenvolvimento e implementa√ß√£o de modelos de Aprendizado de M√°quina."
  },
  {
    "objectID": "index.html#ferramentas.",
    "href": "index.html#ferramentas.",
    "title": "Home",
    "section": "Ferramentas.",
    "text": "Ferramentas."
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference",
    "section": "",
    "text": "Livro 1\n\n\nLivro 2\n\n\nLivro 3"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "",
    "text": "Nosso objetivo √© desenvolver uma aplica√ß√£o web utilizando Flask e Nginx, duas ferramentas populares para cria√ß√£o de servi√ßos web em Python.\n\n\nPara garantir facilidade de distribui√ß√£o e execu√ß√£o consistente, vamos encapsular todo o c√≥digo em containers Docker.\n\n\nAl√©m disso, faremos o deploy da nossa aplica√ß√£o em uma inst√¢ncia EC2 na AWS, aproveitando a escalabilidade e confiabilidade dos servi√ßos de nuvem.\n\n\nEste tutorial fornecer√° um passo a passo claro e direto, ideal para quem deseja aprender a deployar uma aplica√ß√£o web de forma eficiente e r√°pida."
  },
  {
    "objectID": "project5.html#arquitetura-da-aplica√ß√£o",
    "href": "project5.html#arquitetura-da-aplica√ß√£o",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Arquitetura da aplica√ß√£o",
    "text": "Arquitetura da aplica√ß√£o\n\nflask_app_tutorial/\n    README.md\n    flask_app/\n        app/\n            requirements.txt # Depend√™ncias que ser√£o instaladas (Flask, Nginx, ...)\n            Dockerfile       # Imagem Python + depend√™ncias\n            static/          # Pasta onde guardamos os arquivos .css, imagens e √≠cones\n                *.css, *.png \n            templates/       # Pasta onde guardamos as p√°ginas .html\n                *.html       \n            wsgi.py          # Arquivo principal para execu√ß√£o do Flask (o main)\n        create_ssh_key.sh    # Bash script para cria√ß√£o de chave ssh a qual dever√° ser colocada no github\n        install_docker.sh    # Bash script para instala√ß√£o do Docker\n        nginx.conf           # Configura√ß√£o do Nginx\n        docker-compose.yml   # Orquestra√ß√£o dos containers\n\n\nSupondo que temos a aplica√ß√£o funcionando localmente, a qual est√° sendo executada na porta 5000, conforme demonstra o c√≥digo contido em wsgi.py\n\n\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n@app.route(\"/projects\")\ndef projects():\n    return render_template(\"projects.html\")\n\n@app.route(\"/project1\")\ndef project1():\n    return render_template(\"project1.html\")\n\n@app.route(\"/project2\")\ndef project2():\n    return render_template(\"project2.html\")\n\n@app.route(\"/project3\")\ndef project3():\n    return render_template(\"project3.html\")\n\n@app.route(\"/project4\")\ndef project4():\n    return render_template(\"project4.html\")\n\n@app.route(\"/project5\")\ndef project5():\n    return render_template(\"project5.html\")\n\n        \nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)"
  },
  {
    "objectID": "project5.html#cria√ß√£o-do-arquivo-de-configura√ß√£o-do-servidor-nginx",
    "href": "project5.html#cria√ß√£o-do-arquivo-de-configura√ß√£o-do-servidor-nginx",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Cria√ß√£o do arquivo de configura√ß√£o do servidor Nginx",
    "text": "Cria√ß√£o do arquivo de configura√ß√£o do servidor Nginx\n\nVamos agora criar o arquivo de configura√ß√£o do servidor web (nginx.conf).\n\n\nQuando o servidor entrar em execu√ß√£o a aplica√ß√£o que est√° sendo executada na porta 5000 ser√° executada na porta 80 pelo servidor.\n\n\nevents {\n    worker_connections 1000;\n}\n\nhttp {\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://app:5000;\n        }\n    }\n}"
  },
  {
    "objectID": "project5.html#cria√ß√£o-do-dockerfile",
    "href": "project5.html#cria√ß√£o-do-dockerfile",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Cria√ß√£o do Dockerfile",
    "text": "Cria√ß√£o do Dockerfile\n\nVamos agora criar o arquivo de configura√ß√£o docker, no qual vamos definir o uso de uma imagem Python e tamb√©m vamos determinar a instala√ß√£o do Flask e as demais depend√™ncias nesta imagem.\n\n\n# Use uma imagem base do Python\nFROM python:3.9\n\n# Defina o diret√≥rio de trabalho como /app no docker\nWORKDIR /app\n\n# Copie todos os arquivos para o diret√≥rio de trabalho\nCOPY . .\n\n# Instale as depend√™ncias do Flask especificadas no arquivo requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Execute wsgi.py:app()\nCMD gunicorn --bind 0.0.0.0:5000 wsgi:app"
  },
  {
    "objectID": "project5.html#cria√ß√£o-do-docker-compose",
    "href": "project5.html#cria√ß√£o-do-docker-compose",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Cria√ß√£o do docker-compose",
    "text": "Cria√ß√£o do docker-compose\n\nCria√ß√£o do arquivo para orquestra√ß√£o dos containers.\n\n\nVamos levantar dois containers, um com o build da imagem python (dockerfile mostrado acima) e o outro com uma imagem direta do servidor Nginx.\n\n\nversion: \"3\"\n\nservices:\n  app:\n    build:\n      context: app\n    ports:\n      - \"5000\"\n  \n  nginx:\n    image: nginx:latest\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro #Considerando que a m√°quina EC2 ser√° um ubuntu. \n    depends_on:\n      - app\n    ports:\n      - \"80:80\""
  },
  {
    "objectID": "project5.html#cria√ß√£o-da-m√°quina-ec2",
    "href": "project5.html#cria√ß√£o-da-m√°quina-ec2",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Cria√ß√£o da m√°quina EC2",
    "text": "Cria√ß√£o da m√°quina EC2\n\nN√£o entraremos no detalhe de como criar uma m√°quina EC2, visto que j√° existem in√∫meros tutoriais.\n\n\nDesta forma deixamos uma excelente refer√™ncia dispon√≠vel em https://www.youtube.com/watch?v=a6nU5NTHJDM&list=PLOF5f9_x-OYUaqJar6EKRAonJNSHDFZUm&index=9"
  },
  {
    "objectID": "project5.html#gera√ß√£o-de-chave-ssh",
    "href": "project5.html#gera√ß√£o-de-chave-ssh",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Gera√ß√£o de chave SSH",
    "text": "Gera√ß√£o de chave SSH\n\nDentro do EC2 geramos uma chave ssh e colocamos no github para autorizar o acesso o reposit√≥rio no git.\n\n\nO script abaixo est√° contido em create_ssh_key.sh\n\n\nssh-keygen -t rsa\ncat ~/.ssh/id_rsa.pub\n\nUma vez feito isso, podemos clonar o reposit√≥rio."
  },
  {
    "objectID": "project5.html#instala√ß√£o-do-docker",
    "href": "project5.html#instala√ß√£o-do-docker",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Instala√ß√£o do Docker",
    "text": "Instala√ß√£o do Docker\n\nInstalamos o docker e alteramos o grupo dele para conseguirmos utilizar sem a necessidade de acrescentar sudo em cada comando.\n\n\nO script abaixo est√° contido em install_docker.sh\n\n\n###############################################################\n#                   INSTALA√á√ÉO DO DOCKER                      #\n###############################################################\n\n# Adicione a chave GPG oficial do Docker:\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Add o repo ao apt sources:\necho \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Instalando os packages do docker.\nsudo apt-get install     \\\n    docker-ce            \\\n    docker-ce-cli        \\\n    containerd.io        \\\n    docker-buildx-plugin \\ docker-compose-plugin\n\n\n###############################################################\n#              CRIA√á√ÉO DO GRUPO DO DOCKER E SEU USER         #\n###############################################################\n\n# Cria√ß√£o do grupo do docker\nsudo groupadd docker\n\n# Add seu user ao grupo do docker\nsudo usermod -aG docker $USER\n\n# Ativando as mudan√ßas no grupo\nnewgrp docker"
  },
  {
    "objectID": "project5.html#execu√ß√£o-do-docker-compose",
    "href": "project5.html#execu√ß√£o-do-docker-compose",
    "title": "Projeto: Deploy de Uma Aplica√ß√£o Web (Flask + Nginx + Docker + AWS)",
    "section": "Execu√ß√£o do docker compose",
    "text": "Execu√ß√£o do docker compose\n\nUma vez com o docker instalado e o reposit√≥rio do github clonado, precisamos somente subir os containers com o comanda baixo que a nossa aplica√ß√£o estar√° disponivel na porta 80 do EC2.\n\n\ndocker compose ud -d"
  },
  {
    "objectID": "project6.html",
    "href": "project6.html",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "",
    "text": "O Spark e o PySpark s√£o ferramentas de grande destaque no ecossistema de big data e an√°lise de dados.\n\n\nO Apache Spark √© um mecanismo de processamento de dados em cluster que fornece uma plataforma unificada para processamento de dados distribu√≠dos em larga escala. Ele √© altamente eficiente, oferecendo suporte a diversas linguagens de programa√ß√£o, como Scala, Java, Python e SQL.\n\n\nJ√° o PySpark √© a interface Python para o Spark, permitindo aos desenvolvedores escreverem c√≥digo em Python para acessar a funcionalidade do Spark.\n\n\nA import√¢ncia dessas ferramentas reside na capacidade de lidar com grandes volumes de dados de forma eficiente, possibilitando o processamento r√°pido e a an√°lise de informa√ß√µes em tempo real.\n\n\nO Spark e o PySpark s√£o amplamente utilizados em uma variedade de aplica√ß√µes, desde an√°lise de dados, aprendizado de m√°quina, processamento de streaming at√© business intelligence, proporcionando escalabilidade e desempenho para projetos de an√°lise de dados de grande porte.\n\n\nEste projeto tem como objetivo apresentar de maneira simples, por√©m funcional, como podemos realizar uma an√°lise de dados utilizando o PySpark. Para isso, faremos uso dos dados de pre√ßos de combust√≠veis de janeiro a abril de 2024, os quais est√£o hospedados em https://dados.gov.br/dados/conjuntos-dados/serie-historica-de-precos-de-combustiveis-e-de-glp?source=post_page-----4185005771e5."
  },
  {
    "objectID": "project6.html#setup-e-carga-dos-dados",
    "href": "project6.html#setup-e-carga-dos-dados",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "Setup e Carga dos Dados",
    "text": "Setup e Carga dos Dados\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n# Setup\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nimport os\n\nIniciando uma sess√£o local com todos os cores dispon√≠veis.\n\n# Session\nspark = ( \n SparkSession\n .builder\n    .master(\"local[*]\")\n .appName('spark_dataframe_api')\n .getOrCreate()\n)\n\nCriando um dataframe Spark a partir dos arquivos csv contidos em ‚Äú./source/project6/‚Äù.\n\n# Criando o dataframe spark\ndf = (\n    spark\n    .read\n    .option('delimiter', ';')\n    .option('header', 'true')\n    .option('inferSchema', 'true') # Vamos deixar o spark inferir o schema\n    .option('enconding', 'ISO-8859-1')\n    .csv([\"./source/project6/\" + file for file in os.listdir(\"./source/project6/\")])\n)\n\n# Schema criado\ndf.printSchema()\n\nroot\n |-- Regiao - Sigla: string (nullable = true)\n |-- Estado - Sigla: string (nullable = true)\n |-- Municipio: string (nullable = true)\n |-- Revenda: string (nullable = true)\n |-- CNPJ da Revenda: string (nullable = true)\n |-- Nome da Rua: string (nullable = true)\n |-- Numero Rua: string (nullable = true)\n |-- Complemento: string (nullable = true)\n |-- Bairro: string (nullable = true)\n |-- Cep: string (nullable = true)\n |-- Produto: string (nullable = true)\n |-- Data da Coleta: string (nullable = true)\n |-- Valor de Venda: string (nullable = true)\n |-- Valor de Compra: string (nullable = true)\n |-- Unidade de Medida: string (nullable = true)\n |-- Bandeira: string (nullable = true)"
  },
  {
    "objectID": "project6.html#preprocessamento-dos-dados",
    "href": "project6.html#preprocessamento-dos-dados",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "Preprocessamento dos dados",
    "text": "Preprocessamento dos dados\nDefinindo as colunas que vamos manter no dataframe.\n\n# Colunas que vamos manter\nkeep_cols = [\n    'Regiao - Sigla',\n    'Estado - Sigla',\n    'Produto',\n    'Data da Coleta',\n    'Valor de Venda',\n    ]\n\nProcessando os dados.\n\n# Preprocessamento\ndf = (\n      df\n      \n      # Filtrando algumas colunas\n      .select(keep_cols) \n      \n      # Substituindo , por . e alterando a tipagem para float\n      .withColumn(\n          \"Valor de Venda\",\n          F.regexp_replace(F.col(\"Valor de Venda\"), \",\", \".\")\n          .cast(\"float\")\n      )\n      \n      # String em data\n      .withColumn(\n          \"Mes\", F.to_timestamp('Data da Coleta', 'dd/MM/yyyy')\n      )\n      \n      # Pegando somente o mes\n      .withColumn(\n          \"Mes\", F.month('Mes')\n      )\n  )"
  },
  {
    "objectID": "project6.html#an√°lise-explorat√≥ria",
    "href": "project6.html#an√°lise-explorat√≥ria",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "An√°lise Explorat√≥ria",
    "text": "An√°lise Explorat√≥ria\nAvaliando se alguma coluna tem registros nulos.\n\n# Quantidade de registros nulos em cada coluna --&gt; Nenhum\nfor colname in df.columns:\n    print(f\"\"\"Col: {colname} - Qtd Nulos: {df.where(F.col(colname).isNull() | F.isnan(colname)).count()}\"\"\")\n\nCol: Regiao - Sigla - Qtd Nulos: 0\nCol: Estado - Sigla - Qtd Nulos: 0\nCol: Produto - Qtd Nulos: 0\nCol: Data da Coleta - Qtd Nulos: 0\nCol: Valor de Venda - Qtd Nulos: 0\nCol: Mes - Qtd Nulos: 0\n\n\nUma amostra do nosso dataframe ap√≥s processamento.\n\ndf.show(5)\n\n+--------------+--------------+------------------+--------------+--------------+---+\n|Regiao - Sigla|Estado - Sigla|           Produto|Data da Coleta|Valor de Venda|Mes|\n+--------------+--------------+------------------+--------------+--------------+---+\n|             N|            AC|          GASOLINA|    01/02/2024|          7.45|  2|\n|            NE|            CE|          GASOLINA|    01/02/2024|          5.95|  2|\n|            NE|            CE|GASOLINA ADITIVADA|    01/02/2024|          6.15|  2|\n|            NE|            CE|            ETANOL|    01/02/2024|          3.99|  2|\n|            NE|            CE|          GASOLINA|    01/02/2024|          5.93|  2|\n+--------------+--------------+------------------+--------------+--------------+---+\nonly showing top 5 rows\n\n\n\nValores distintos em Regi√£o.\n\ndf.select(\"Regiao - Sigla\").distinct().show()\n\n+--------------+\n|Regiao - Sigla|\n+--------------+\n|            NE|\n|             N|\n|             S|\n|            SE|\n|            CO|\n+--------------+\n\n\n\nValores distintos em Estado.\n\ndf.select(\"Estado - Sigla\").distinct().show()\n\n+--------------+\n|Estado - Sigla|\n+--------------+\n|            SC|\n|            RO|\n|            PI|\n|            AM|\n|            RR|\n|            GO|\n|            TO|\n|            MT|\n|            SP|\n|            ES|\n|            PB|\n|            RS|\n|            MS|\n|            AL|\n|            MG|\n|            PA|\n|            BA|\n|            SE|\n|            PE|\n|            CE|\n+--------------+\nonly showing top 20 rows\n\n\n\nValores distintos em Produto.\n\ndf.select(\"Produto\").distinct().show()\n\n+------------------+\n|           Produto|\n+------------------+\n|GASOLINA ADITIVADA|\n|            ETANOL|\n|          GASOLINA|\n+------------------+\n\n\n\nValores distintos em Mes.\n\ndf.select(\"Mes\").distinct().show()\n\n+---+\n|Mes|\n+---+\n|  2|\n|  1|\n|  3|\n|  4|\n+---+"
  },
  {
    "objectID": "project6.html#criando-novas-features",
    "href": "project6.html#criando-novas-features",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "Criando Novas Features",
    "text": "Criando Novas Features\nAgrupando por regi√£o, estado e produto.\n\n# Agrega√ß√£o\ndf_all = (\n    df\n\n    # Agrupando por\n    .groupBy(\n        F.col(\"Regiao - Sigla\"),\n        F.col(\"Estado - Sigla\"),\n        F.col(\"Produto\")\n    )\n\n    # Forma de agrega√ß√£o\n    .agg(\n\n        # Min\n        F.min(\"Valor de Venda\").alias(\"min_vlr_venda\"),\n        \n        # M√©dia\n        F.mean(\"Valor de Venda\").alias(\"media_vlr_venda\"),\n\n        # Max\n        F.max(\"Valor de Venda\").alias(\"max_vlr_venda\")\n\n    )\n    .withColumn(\"variacao\", F.col(\"max_vlr_venda\") - F.col(\"min_vlr_venda\"))\n    .withColumn(\"variacao\", F.format_number(\"variacao\", 2))\n    .withColumn(\"media_vlr_venda\", F.format_number(\"media_vlr_venda\", 2))\n\n    .orderBy(\n        \"variacao\", ascending = False\n    )\n)\n\ndf_all.show()\n\n+--------------+--------------+------------------+-------------+---------------+-------------+--------+\n|Regiao - Sigla|Estado - Sigla|           Produto|min_vlr_venda|media_vlr_venda|max_vlr_venda|variacao|\n+--------------+--------------+------------------+-------------+---------------+-------------+--------+\n|            SE|            SP|GASOLINA ADITIVADA|         4.69|           5.85|         8.89|    4.20|\n|            SE|            SP|          GASOLINA|         4.55|           5.57|         7.99|    3.44|\n|             N|            PA|            ETANOL|         3.45|           4.37|          6.6|    3.15|\n|            SE|            SP|            ETANOL|         2.63|           3.44|         5.69|    3.06|\n|            SE|            RJ|          GASOLINA|         4.84|           5.75|         7.59|    2.75|\n|            SE|            RJ|GASOLINA ADITIVADA|         5.15|           5.91|         7.79|    2.64|\n|             N|            AM|          GASOLINA|         5.19|           6.37|          7.8|    2.61|\n|            NE|            AL|            ETANOL|         3.19|           4.17|          5.8|    2.61|\n|             N|            AM|GASOLINA ADITIVADA|         5.19|           6.28|         7.69|    2.50|\n|            NE|            BA|            ETANOL|         3.09|           4.29|         5.59|    2.50|\n|            SE|            MG|            ETANOL|         2.79|           3.63|         5.25|    2.46|\n|             S|            PR|            ETANOL|         2.99|           3.82|         5.39|    2.40|\n|             S|            RS|            ETANOL|         3.65|           4.47|         5.99|    2.34|\n|            NE|            PE|            ETANOL|         3.39|           4.13|         5.69|    2.30|\n|            NE|            PE|          GASOLINA|         4.75|           5.71|         6.99|    2.24|\n|             S|            RS|          GASOLINA|         5.03|           5.70|         7.24|    2.21|\n|            NE|            CE|            ETANOL|         3.75|           4.52|         5.95|    2.20|\n|            NE|            RN|            ETANOL|         3.39|           4.43|         5.59|    2.20|\n|             S|            RS|GASOLINA ADITIVADA|         5.03|           5.89|         7.23|    2.20|\n|             S|            SC|            ETANOL|         3.62|           4.25|         5.79|    2.17|\n+--------------+--------------+------------------+-------------+---------------+-------------+--------+\nonly showing top 20 rows\n\n\n\nAgrupando por regi√£o, estado, produto e m√™s.\n\n# Agrega√ß√£o\ndf_by_month = (\n    df\n\n    # Agrupando por\n    .groupBy(\n        F.col(\"Regiao - Sigla\"),\n        F.col(\"Estado - Sigla\"),\n        F.col(\"Produto\"),\n        F.col(\"Mes\")\n    )\n\n    # Forma de agrega√ß√£o\n    .agg(\n\n        # Min\n        F.min(\"Valor de Venda\").alias(\"min_vlr_venda\"),\n        \n        # M√©dia\n        F.mean(\"Valor de Venda\").alias(\"media_vlr_venda\"),\n\n        # Max\n        F.max(\"Valor de Venda\").alias(\"max_vlr_venda\")\n\n    )\n    .withColumn(\"variacao\", F.col(\"max_vlr_venda\") - F.col(\"min_vlr_venda\"))\n    .withColumn(\"variacao\", F.format_number(\"variacao\", 2))\n    .withColumn(\"media_vlr_venda\", F.format_number(\"media_vlr_venda\", 2))\n\n    .orderBy(\n        \"variacao\", ascending = False\n    )\n)\n\ndf_by_month.show()\n\n+--------------+--------------+------------------+---+-------------+---------------+-------------+--------+\n|Regiao - Sigla|Estado - Sigla|           Produto|Mes|min_vlr_venda|media_vlr_venda|max_vlr_venda|variacao|\n+--------------+--------------+------------------+---+-------------+---------------+-------------+--------+\n|            SE|            SP|GASOLINA ADITIVADA|  4|         4.69|           5.91|         8.89|    4.20|\n|            SE|            SP|GASOLINA ADITIVADA|  2|         4.79|           5.87|         8.89|    4.10|\n|            SE|            SP|GASOLINA ADITIVADA|  1|         4.69|           5.75|         8.69|    4.00|\n|            SE|            SP|          GASOLINA|  1|         4.55|           5.47|         7.99|    3.44|\n|            SE|            SP|          GASOLINA|  2|         4.59|           5.59|         7.97|    3.38|\n|            SE|            SP|          GASOLINA|  3|         4.59|           5.61|         7.97|    3.38|\n|            SE|            SP|          GASOLINA|  4|         4.69|           5.63|         7.97|    3.28|\n|            SE|            SP|GASOLINA ADITIVADA|  3|         4.79|           5.88|         7.99|    3.20|\n|             N|            PA|            ETANOL|  1|         3.59|           4.46|          6.6|    3.01|\n|            SE|            SP|            ETANOL|  3|         2.69|           3.43|         5.69|    3.00|\n|             N|            PA|            ETANOL|  2|         3.65|           4.36|          6.6|    2.95|\n|            SE|            SP|            ETANOL|  1|         2.63|           3.27|         5.49|    2.86|\n|             N|            AM|          GASOLINA|  2|         5.29|           6.33|          7.8|    2.51|\n|             N|            AM|          GASOLINA|  1|         5.19|           6.08|          7.7|    2.51|\n|             N|            AM|GASOLINA ADITIVADA|  1|         5.19|           6.03|         7.69|    2.50|\n|            SE|            RJ|          GASOLINA|  3|         4.99|           5.78|         7.49|    2.50|\n|            SE|            RJ|GASOLINA ADITIVADA|  4|         5.29|           5.96|         7.79|    2.50|\n|            SE|            MG|            ETANOL|  1|         2.79|           3.44|         5.25|    2.46|\n|             N|            PA|            ETANOL|  3|         3.45|           4.28|          5.9|    2.45|\n|            SE|            SP|            ETANOL|  4|         2.85|           3.63|         5.29|    2.44|\n+--------------+--------------+------------------+---+-------------+---------------+-------------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "project6.html#respondendo-algumas-quest√µes",
    "href": "project6.html#respondendo-algumas-quest√µes",
    "title": "Projeto: An√°lise de Dados com Spark - PySpark",
    "section": "Respondendo Algumas Quest√µes",
    "text": "Respondendo Algumas Quest√µes\nQual a regi√£o com maior pre√ßo por lito de Gasolina ?\n\n# Filtrando pela gasolina\ndf_gasolina = df_all.filter((F.col(\"Produto\") ==  \"GASOLINA\"))\n\n# Obtendo o valor m√°ximo\nmax_value   = df_gasolina.selectExpr(\"max(media_vlr_venda)\").collect()[0][0]\n\n# Obtendo a regi√£o associada ao valor m√°ximo do litro de gasolina\ndf_gasolina.filter(F.col(\"media_vlr_venda\") == max_value).select(\"Regiao - Sigla\").show()\n\n+--------------+\n|Regiao - Sigla|\n+--------------+\n|             N|\n+--------------+\n\n\n\nQual o estado com maior pre√ßo por lito de Gasolina Aditivada ?\n\n# Filtrando pela gasolina aditivada\ndf_gasolina_ad = df_all.filter((F.col(\"Produto\") ==  \"GASOLINA ADITIVADA\"))\n\n# Obtendo o valor m√°ximo\nmax_value   = df_gasolina_ad.selectExpr(\"max(media_vlr_venda)\").collect()[0][0]\n\n# Obtendo o estado associado ao valor m√°ximo do litro de gasolina aditivada\ndf_gasolina_ad.filter(F.col(\"media_vlr_venda\") == max_value).select(\"Estado - Sigla\").show()\n\n+--------------+\n|Estado - Sigla|\n+--------------+\n|            AC|\n+--------------+\n\n\n\nQual o produto com maior varia√ß√£o de pre√ßo em SP ?\n\n# Filtrando pelo estado de SP\ndf_SP = df_all.filter((F.col(\"Estado - Sigla\") ==  \"SP\"))\n\n# Obtendo o valor m√°ximo\nmax_value = df_SP.selectExpr(\"max(variacao)\").collect()[0][0]\n\n# Obtendo o produto associado √† vari√ß√£o m√°xima\ndf_SP.filter(F.col(\"variacao\") == max_value).select(\"Produto\").show()\n\n+------------------+\n|           Produto|\n+------------------+\n|GASOLINA ADITIVADA|\n+------------------+\n\n\n\nEm qual m√™s o etanol no PR obteve sua maior alta ?\n\n# Filtrando pelo etanol no PR\ndf_etanol_PR = df_by_month.filter(\n    (F.col(\"Produto\") ==  \"ETANOL\") &\n    (F.col(\"ESTADO - Sigla\") ==  \"PR\") \n    )\n\n# Obtendo o valor m√°ximo\nmax_value   = df_etanol_PR.selectExpr(\"max(media_vlr_venda)\").collect()[0][0]\n\n# Obtendo a regi√£o associada ao valor m√°ximo do litro de etanol\ndf_etanol_PR.filter(F.col(\"media_vlr_venda\") == max_value).select(\"Mes\").show()\n\n+---+\n|Mes|\n+---+\n|  4|\n+---+"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "",
    "text": "O problema de churn, ou rotatividade de clientes, representa um desafio significativo para institui√ß√µes banc√°rias. Refere-se √† taxa na qual os clientes encerram seus relacionamentos com o banco, migrando para outras institui√ß√µes financeiras ou at√© mesmo abandonando servi√ßos financeiros. Este fen√¥meno pode ser impulsionado por diversos fatores, como insatisfa√ß√£o com servi√ßos, concorr√™ncia acirrada, mudan√ßas nas condi√ß√µes econ√¥micas e at√© mesmo avan√ßos tecnol√≥gicos.\n\n\nDessa forma, a reten√ß√£o de clientes torna-se uma prioridade estrat√©gica para os bancos, que buscam constantemente inovar, oferecer experi√™ncias mais atrativas e personalizadas, a fim de mitigar o churn e manter uma base s√≥lida de clientes leais.\n\n\nDiante desse cen√°rio, torna-se de extrema valia a capacidade de identificar os clientes mais propensos ao churn, possibilitando que a institui√ß√£o financeira aja de maneira antecipada, oferecendo planos e servi√ßos atrativos que impe√ßam a fuga do cliente.\n\n\nNeste projetos, temos como objetivo demonstrar a constru√ß√£o de modelo de ML para Identificar os clientes mais propensos ao churn no contexto banc√°rio."
  },
  {
    "objectID": "project2.html#an√°lise-explorat√≥ria-dos-dados-eda",
    "href": "project2.html#an√°lise-explorat√≥ria-dos-dados-eda",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "An√°lise Explorat√≥ria dos Dados (EDA)",
    "text": "An√°lise Explorat√≥ria dos Dados (EDA)\n\nO objetivo desta etapa √© investigar e entender os dados dispon√≠veis, a fim de extrair informa√ß√µes preliminares, identificar padr√µes, tend√™ncias, anomalias e insights relevantes.\n\n\nVerificaremos que, ao t√©rmino dessa an√°lise, todas as vari√°veis apresentam comportamento adequado para prosseguirem no estudo, pois:\n\nN√£o possuem valores faltantes.\nN√£o t√™m valores inesperados.\nN√£o exibem alta concentra√ß√£o em um √∫nico valor.\n\n\n\n\nChurn\nEssa vari√°vel denota a ocorr√™ncia ou n√£o do evento de churn.\n\nN√£o ocorr√™ncia de churn = 0\nOcorr√™ncia de churn = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel target (bin√°ria).\n\n\nTemos 20% de ocorr√™ncia do evento.\n\n\nTemos 80% de n√£o ocorr√™ncia do evento.\n\n\n\ncreate_table_categorical(\"churn\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n7963\n7963\n0.80\n0.80\n\n\n\n1\n2037\n10000\n0.20\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"churn\", df)\n\n\n\n\n\n\n\n\n\n\n\nCountry\nPa√≠s do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nFran√ßa representa 50%.\n\n\nAlemanha e Espanha representam 25% cada.\n\n\n\ncreate_table_categorical(\"country\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\nFrance\n5014\n5014\n0.50\n0.50\n\n\n\nGermany\n2509\n7523\n0.25\n0.75\n\n\n\nSpain\n2477\n10000\n0.25\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"country\", df)\n\n\n\n\n\n\n\n\n\n\n\nGender\nG√™nero do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n55% Homens.\n\n\n45% Mulheres.\n\n\n\ncreate_table_categorical(\"gender\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\nFemale\n4543\n4543\n0.45\n0.45\n\n\n\nMale\n5457\n10000\n0.55\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"gender\", df)\n\n\n\n\n\n\n\n\n\n\n\nCredit Card\nSe o cliente utiliza cart√£o de cr√©dito.\n\nN√£o utiliza cart√£o de cr√©dito = 0\nUtiliza cart√£o de cr√©dito = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n71% utiliza cart√£o de c≈ïedito.\n\n\n29% n√£o utiliza.\n\n\n\ncreate_table_categorical(\"credit_card\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n2945\n2945\n0.29\n0.29\n\n\n\n1\n7055\n10000\n0.71\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"credit_card\", df)\n\n\n\n\n\n\n\n\n\n\n\nActive Member\nDenota se um cliente est√° envolvido e participando ativamente das atividades e servi√ßos oferecidos pelo banco.\n\nN√£o ativo = 0\nAtivo = 1\n\nAvalia√ß√£o:  OK. \n\n\nVari√°vel categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPraticamente metade dos clientes s√£o ativos e a outra metade √© inativo.\n\n\n\ncreate_table_categorical(\"active_member\", df)\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n0\n4849\n4849\n0.48\n0.48\n\n\n\n1\n5151\n10000\n0.52\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"active_member\", df)\n\n\n\n\n\n\n\n\n\n\n\nProducts Number\nN√∫mero de produtos adiquiridos pelo cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica discreta, mas ser√° visualizada como categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n~50% dos cliente utilizam somente um produto.\n\n\n~46% dos cliente utilizam 2 produtos.\n\n\n\ncreate_table_categorical(\"products_number\", df, ['1', '2', '3', '&gt;=4'])\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n1\n5084\n5084\n0.51\n0.51\n\n\n\n2\n4590\n9674\n0.46\n0.97\n\n\n\n3\n266\n9940\n0.03\n0.99\n\n\n\n&gt;=4\n60\n10000\n0.01\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"products_number\", df, ['1', '2', '3', '&gt;=4'])\n\n\n\n\n\n\n\n\n\n\n\nTenure\nMensura a quantidade de anos decorridos desde que o cliente aderiu aos servi√ßos prestados pelo banco.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica discreta, mas ser√° visualizada como categ√≥rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\n25% de 0 a 2 anos.\n\n\n15% de 8 a 10 anos.\n\n\nAs demais classes est√£o bem distribu√≠dos em torno de 20%.\n\n\n\n#fazer a mesma orden√ß√£o de baixo aqui\ncreate_table_categorical(\"tenure\", df, ['[0, 2]', '(2, 4]', '(4, 6]', '(6, 8]', '(8, 10]'])\n\n\n\n\n\n\n¬†\nClass\nAbs. Freq.\nAcc. Abs. Freq.\nRel. Freq.\nAcc. Rel. Freq.\n\n\n\n\n\n[0, 2]\n2496\n2496\n0.25\n0.25\n\n\n\n(2, 4]\n1998\n4494\n0.20\n0.45\n\n\n\n(4, 6]\n1979\n6473\n0.20\n0.65\n\n\n\n(6, 8]\n2053\n8526\n0.21\n0.85\n\n\n\n(8, 10]\n1474\n10000\n0.15\n1.00\n\n\n\n\n\n\ncreate_graph_categorical(\"tenure\", df, ['[0, 2]', '(2, 4]', '(4, 6]', '(6, 8]', '(8, 10]'])\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\n√â uma medida da probabilidade de um indiv√≠duo pagar suas d√≠vidas com base em seu hist√≥rico de cr√©dito passado.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPossui distribui√ß√£o aproximadamente normal, com m√©dia = 650.\n\n\n\ncreate_table_numeric_continuous(\"credit_score\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n6,505,288.00\n10000\n650.53\n96.65\n0.00\n0.00\n350.00\n432.00\n584.00\n652.00\n718.00\n778.00\n812.00\n850.00\n850.00\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"credit_score\", df)\n\n\n\n\n\n\n\n\n\n\n\nAge\nIdade em anos do cliente.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nPossui distribui√ß√£o assim√©trica √° direita.\n\n\nCliente mais novo tem 18 anos.\n\n\nCliente mais velho tem 92 anos.\n\n\nO valor mediano da idade do cliente √© de 37 anos.\n\n\n\ncreate_table_numeric_continuous(\"age\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n389,218.00\n10000\n38.92\n10.49\n0.00\n0.00\n18.00\n21.00\n32.00\n37.00\n44.00\n53.00\n60.00\n72.00\n92.00\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"age\", df)\n\n\n\n\n\n\n\n\n\n\n\nBalance\nSaldo banc√°rio do cliente\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nVari√°vel com distribui√ß√£o assim√©trica.\n\n\nAlta concetra√ß√£o de clientes com saldo entre 0 e 25 mil.\n\n\n\ncreate_table_numeric_continuous(\"balance\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n764,858,892.88\n10000\n76,485.89\n62,397.41\n0.36\n0.00\n0.00\n0.00\n0.00\n97,198.54\n127,644.24\n149,244.79\n162,711.67\n185,967.99\n250,898.09\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"balance\", df)\n\n\n\n\n\n\n\n\n\n\n\nEstimated Salary\nSalario estimado.\nAvalia√ß√£o:  OK. \n\n\nVari√°vel num√©rica.\n\n\nN√£o possui nenhum valor faltante.\n\n\nVari√°vel com distribui√ß√£o uniforme.\n\n\n\ncreate_table_numeric_continuous(\"estimated_salary\", df)\n\n\n\n\n\n\n¬†\nSUM\nCNT\nAVG\nSTDEV\nPERC_zeros\nPERC_negatives\nMIN\nP1\nP25\nP50\nP75\nP90\nP95\nP99\nMAX\n\n\n\n\n\n1,000,902,398.81\n10000\n100,090.24\n57,510.49\n0.00\n0.00\n11.58\n1,842.83\n51,002.11\n100,193.92\n149,388.25\n179,674.70\n190,155.38\n198,069.73\n199,992.48\n\n\n\n\n\n\ncreate_graph_numeric_continuous(\"estimated_salary\", df)"
  },
  {
    "objectID": "project2.html#an√°lise-bivariada-dos-dados",
    "href": "project2.html#an√°lise-bivariada-dos-dados",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "An√°lise Bivariada dos Dados",
    "text": "An√°lise Bivariada dos Dados\n\nA an√°lise bivariada tem como objetivo examinar a rela√ß√£o entre duas vari√°veis em um conjunto de dados. Ao contr√°rio da an√°lise univariada, que se concentra em uma √∫nica vari√°vel (EDA que fizemos no item anterior), a an√°lise bivariada explora a associa√ß√£o entre duas vari√°veis.\n\n\nExistem diferentes t√©cnicas e m√©todos para realizar uma an√°lise bivariada, dependendo da natureza das vari√°veis envolvidas. Para o nosso problema, estamos interessados em avaliar o grau de associa√ß√£o entre cada uma das poss√≠veis vari√°veis preditoras e a vari√°vel target, dessa forma temos que:\n\nchurn: Categ√≥rica Bin√°ria (target)\ncountry: Categ√≥rica\ngender: Categ√≥rica\ncredit_card: Categ√≥rica\nactive_member: Categ√≥rica\nproducs_number: Num√©rica Discreta\ntenure: Num√©rica Discreta\ncredit_score: Num√©rica Cont√≠nua\nage: Num√©rica Discreta\nbalance: Num√©rica Cont√≠nua\nestimated_salary: Num√©rica Cont√≠nua\n\n\n\nA nossa abordagem ser√° transformar todas as vari√°veis preditoras em categ√≥ricas, para posteriormente avaliarmos o grau de associa√ß√£o de cada uma delas frente a vari√°vel target, para isso teremos basicamente duas etapas:\n\n\nBinning das vari√°veis.\n\n\nAvalia√ß√£o do Grau de Associa√ß√£o.\n\n\n\n\nBinning\n\nChamaremos esse processo de transforma√ß√£o de uma vari√°vel num√©rica em categ√≥rica de binning. O processo de binning ser√° feito atrav√©s de um m√©todo denominado de optimal binning https://gnpalencia.org/optbinning/.\n\n\nO optimal binning refere-se a uma abordagem estat√≠stica utilizada em an√°lise de dados para agrupar valores de uma vari√°vel em intervalos (ou ‚Äúbins‚Äù) de maneira a otimizar algum crit√©rio espec√≠fico. A principal ideia por tr√°s do binning √≥timo √© encontrar a divis√£o mais informativa ou significativa das observa√ß√µes, geralmente com base em algum crit√©rio de interesse, como a maximiza√ß√£o da diferen√ßa nas m√©dias entre os grupos ou a minimiza√ß√£o da variabilidade intra-bin.\n\n\nOu seja, de forma resumida, vamos pegar um certa vari√°vel, por exemplo o balance (saldo da conta) e tentar discretizar em categorias onde fique mais evidente se a ocorr√™ncia de churn √© maior ou menor.\n\n\nVale ressaltar que tamb√©m vamos aplicar o optimal binning para as vari√°veis que j√° s√£o categ√≥ricas, uma vez que esse processo de otimiza√ß√£o pode gerar agrupamentos mais informativos (quanto a ocorr√™ncia de churn) do que as categorias j√° existentes.\n\n\nAvalia√ß√£o do Grau de Associa√ß√£o\n\nNeste momento todas as nossas vari√°veis (preditoras e target) s√£o categ√≥ricas, ent√£o para mensurar o grau de associa√ß√£o entre cada preditora e o target, utilizaremos o coeficiente Cramer‚Äôs V (V de Cramer).\n\n\nO coeficiente V de Cramer √© uma medida estat√≠stica utilizada em an√°lises bivariadas para quantificar a for√ßa de associa√ß√£o entre duas vari√°veis categ√≥ricas. Essa medida √© uma extens√£o do coeficiente qui-quadrado, que √© comumente utilizado para testar a independ√™ncia entre vari√°veis categ√≥ricas.\nO coeficiente V de Cramer varia de 0 a 1, onde 0 indica nenhuma associa√ß√£o e 1 indica associa√ß√£o total entre as vari√°veis categ√≥ricas. https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n\n\nResultado da An√°lise Bivariada\nAp√≥s o t√©rmino da an√°lise bivariada, conforme a tabela abaixo e os demais resultados que veremos na sequ√™ncia, veremos que:\n\nage: √â a vari√°vel com maior grau de associa√ß√£o com o evento de churn (Alta discrimin√¢ncia).\nproducts_number, country, active_member: Est√£o associadas de forma moderada com o evento de churn (M√©dia discrimin√¢ncia).\ncredit_score, tenure, estimated_salary e credit_card: Possuem baixo grau de associa√ß√£o com o evento de churn (Baixa discrimin√¢ncia).\n\nAs vari√°veis com alta e m√©dia discrimin√¢ncia t√™m maiores chances de serem consideradas como preditoras no modelo preditivo que iremos construir. Em contrapartida, as vari√°veis de baixa discrimin√¢ncia possuem menor propens√£o de serem utilizadas como preditoras nesse modelo.‚Äù\nObserva√ß√£o\n\nO crit√©rio utilizado para definir a discrimin√¢ncia n√£o est√° levando em conta apenas se o valor de Cramer‚Äôs V est√° muito pr√≥ximo de 0 ou 1, mas tamb√©m considera o contexto dessa an√°lise. Por exemplo, para a vari√°vel idade, temos Cramer‚Äôs V = 0.36, que √© um valor mais pr√≥ximo de 0 do que de 1. Se consider√°ssemos apenas essa quest√£o, dir√≠amos que o grau de associa√ß√£o √© moderado ou baixo.\nNo entanto, no contexto desta an√°lise, a vari√°vel idade √© a que possui o maior Cramer‚Äôs V. Portanto, dentro do nosso contexto, estamos considerando que o grau de associa√ß√£o com o evento √© forte.\nPara fins de esclarecimento, consideramos que:\n\nCramer‚Äôs V &gt; 0.20: Discrimin√¢ncia Alta.\n0.20 &lt;= Cramer‚Äôs V &lt; 0.05: Discrimin√¢ncia M√©dia.\nCramer‚Äôs V &lt;= 0.05: Discrimin√¢ncia Baixa.\n\n\n\nnumerical_variables = [\n    \"products_number\",\n    \"tenure\",\n    \"credit_score\",\n    \"age\",\n    \"estimated_salary\"\n    ]\n\ncategorical_variables = [\n    \"country\", \n    \"gender\", \n    \"credit_card\",\n    \"active_member\",\n    \"balance\",\n    ]\n\ntarget_variable = 'churn'\n\ndf = df_base\n\n# balance\nvar_name = 'balance'\nc1 = df[var_name].between(-np.inf,  1884.34, inclusive = \"neither\")\ndf[var_name] = np.where(c1, '&lt; 1884.34', '&gt;= 1884.34')\n\ndf_bivariate = bivariate(\n    df,\n    numerical_variables,\n    categorical_variables,\n    target_variable)\n\ndf_bivariate.rename(columns = {\"Indicador\":\"Feature\"}, inplace = True)\n\ndf = (\n  df_bivariate[[\"Feature\", \"Cramer's V\", \"Discrimin√¢ncia\"]]\n  .drop_duplicates()\n  .sort_values(by = [\"Cramer's V\", \"Feature\"], ascending = False)\n  .reset_index(drop = True)\n  )\n    \ncreate_table_bivariate_summary(df, cols_float = [\"Cramer's V\"])\n\n\n\n\n\n\n¬†\nFeature\nCramer's V\nDiscrimin√¢ncia\n\n\n\n\n\nage\n0.36\nAlta\n\n\n\nproducts_number\n0.19\nM√©dia\n\n\n\ncountry\n0.17\nM√©dia\n\n\n\nactive_member\n0.16\nM√©dia\n\n\n\nbalance\n0.12\nM√©dia\n\n\n\ngender\n0.11\nM√©dia\n\n\n\ncredit_score\n0.04\nBaixa\n\n\n\ntenure\n0.04\nBaixa\n\n\n\nestimated_salary\n0.03\nBaixa\n\n\n\ncredit_card\n0.01\nBaixa\n\n\n\n\n\n\nAge\nDiscrimin√¢ncia:  Alta. \n\n\nQuanto mais velho for o cliente, maior √© a propens√£o de ocorr√™ncia do churn.\n\n\n\nvar = \"age\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 27.50)\n947\n73\n1020\n7.16\n10.20\n\n\n\n[27.50, 32.50)\n1630\n140\n1770\n7.91\n17.70\n\n\n\n[32.50, 34.50)\n812\n77\n889\n8.66\n8.89\n\n\n\n[34.50, 36.50)\n820\n110\n930\n11.83\n9.30\n\n\n\n[36.50, 38.50)\n830\n125\n955\n13.09\n9.55\n\n\n\n[38.50, 40.50)\n694\n161\n855\n18.83\n8.55\n\n\n\n[40.50, 42.50)\n541\n146\n687\n21.25\n6.87\n\n\n\n[42.50, 46.50)\n670\n339\n1009\n33.60\n10.09\n\n\n\n[46.50, inf)\n1019\n866\n1885\n45.94\n18.85\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nProducts Number\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes que contrataram somente 1 produto.\n\n\n\nvar = \"products_number\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 1.50)\n3675\n1409\n5084\n27.71\n50.84\n\n\n\n[1.50, inf)\n4288\n628\n4916\n12.77\n49.16\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCountry\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes da Alemanha.\n\n\n\nvar = \"country\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['France']\n4204\n810\n5014\n16.15\n50.14\n\n\n\n['Germany']\n1695\n814\n2509\n32.44\n25.09\n\n\n\n['Spain']\n2064\n413\n2477\n16.67\n24.77\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nActive Member\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes que n√£o s√£o ativos.\n\n\n\nvar = \"active_member\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['0']\n3547\n1302\n4849\n26.85\n48.49\n\n\n\n['1']\n4416\n735\n5151\n14.27\n51.51\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nBalance\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes com saldo em conta maior ou igual a 1884.34 .\n\n\n\nvar = \"balance\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['&lt; 1884.34']\n3117\n500\n3617\n13.82\n36.17\n\n\n\n['&gt;= 1884.34']\n4846\n1537\n6383\n24.08\n63.83\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nGender\nDiscrimin√¢ncia:  M√©dia. \n\n\nA ocorr√™ncia do churn √© maior para clientes do sexo feminino.\n\n\n\nvar = \"gender\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['Female']\n3404\n1139\n4543\n25.07\n45.43\n\n\n\n['Male']\n4559\n898\n5457\n16.46\n54.57\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado de qu√£o bom ou ruim √© o credit_score do cliente, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"credit_score\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 489.50)\n377\n125\n502\n24.90\n5.02\n\n\n\n[489.50, 552.50)\n903\n254\n1157\n21.95\n11.57\n\n\n\n[552.50, 629.50)\n1937\n514\n2451\n20.97\n24.51\n\n\n\n[629.50, 651.50)\n696\n181\n877\n20.64\n8.77\n\n\n\n[651.50, 678.50)\n859\n194\n1053\n18.42\n10.53\n\n\n\n[678.50, 703.50)\n782\n162\n944\n17.16\n9.44\n\n\n\n[703.50, 734.50)\n816\n195\n1011\n19.29\n10.11\n\n\n\n[734.50, inf)\n1593\n412\n2005\n20.55\n20.05\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nTenure\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado da quantidade de anos decorridos desde que o cliente aderiu aos servi√ßos prestados pelo banco, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"tenure\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 1.50)\n1121\n327\n1448\n22.58\n14.48\n\n\n\n[1.50, 5.50)\n3232\n826\n4058\n20.35\n40.58\n\n\n\n[5.50, 6.50)\n771\n196\n967\n20.27\n9.67\n\n\n\n[6.50, 7.50)\n851\n177\n1028\n17.22\n10.28\n\n\n\n[7.50, 8.50)\n828\n197\n1025\n19.22\n10.25\n\n\n\n[8.50, inf)\n1160\n314\n1474\n21.30\n14.74\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nEstimated Salary\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado de qu√£o alto ou baixo √© o sal√°rio do cliente, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"estimated_salary\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n(-inf, 33482.46)\n1297\n343\n1640\n20.91\n16.40\n\n\n\n[106162.09, 138559.97)\n1290\n337\n1627\n20.71\n16.27\n\n\n\n[138559.97, 169420.42)\n1184\n310\n1494\n20.75\n14.94\n\n\n\n[169420.42, inf)\n1201\n341\n1542\n22.11\n15.42\n\n\n\n[33482.46, 73970.20)\n1620\n402\n2022\n19.88\n20.22\n\n\n\n[73970.20, 83463.54)\n423\n78\n501\n15.57\n5.01\n\n\n\n[83463.54, 106162.09)\n948\n226\n1174\n19.25\n11.74\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n\n\n\n\n\nCredit Card\nDiscrimin√¢ncia:  Baixa. \n\n\nO fato isolado do cliente ter ou n√£o cart√£o de c≈ïedito, n√£o tem forte rela√ß√£o com o evento de churn.\n\n\n\nvar = \"credit_card\"\ncreate_table_bivariate_html(df_bivariate, var)\n\n\n\n\n\n\n¬†\nCategoria\nN√£o Churn\nChurn\nTotal\n% Churn\n% Categoria\n\n\n\n\n\n['0']\n2332\n613\n2945\n20.81\n29.45\n\n\n\n['1']\n5631\n1424\n7055\n20.18\n70.55\n\n\n\n\n\n\ncreate_graph_bivariate_html(df_bivariate, var)"
  },
  {
    "objectID": "project2.html#o-modelo-preditivo",
    "href": "project2.html#o-modelo-preditivo",
    "title": "Projeto: Previs√£o de Rotatividade de Clientes Banc√°rios",
    "section": "O Modelo Preditivo",
    "text": "O Modelo Preditivo\nEm desenvolvimento - N√£o est√° pronto ainda\nT√©cnica Utilizada\n\nQuanto √† modelagem, um ponto deve ser enfatizado: o principal objetivo aqui √© a constru√ß√£o de um modelo preditivo que possua um grau de interpretabilidade relativamente f√°cil para a √°rea de neg√≥cios. Portanto, nosso foco √© obter n√£o apenas um modelo assertivo, mas sim um modelo com bom desempenho e de f√°cil entendimento. Dito isso, vamos modelar utilizando a t√©cnica de Regress√£o Log√≠stica.\n\n\nA regress√£o log√≠stica √© uma t√©cnica estat√≠stica utilizada para modelar a rela√ß√£o entre uma vari√°vel dependente bin√°ria (que possui apenas dois valores poss√≠veis, geralmente 0 e 1) e uma ou mais vari√°veis independentes. Ela √© amplamente empregada em problemas de classifica√ß√£o, onde o objetivo √© prever a probabilidade de uma observa√ß√£o pertencer a uma determinada categoria.\n\n\nA principal caracter√≠stica da regress√£o log√≠stica √© sua capacidade de lidar com problemas de classifica√ß√£o bin√°ria, como por exemplo, prever se um e-mail √© spam ou n√£o, se um paciente tem uma determinada condi√ß√£o m√©dica ou n√£o, entre outros cen√°rios onde a resposta desejada √© dicot√¥mica.\n\n\nA forma b√°sica da regress√£o log√≠stica √© expressa pela seguinte equa√ß√£o:\n\n\n\n\n\nContextualizando a formula acima para o nosso problema, temos que:\n\nX1, X2, ‚Ä¶ Xn s√£o as nossas features: country, gender, credit_card, active_member, producs_number, tenure, credit_score, age, balance, estimated_salary.\nA probabilide p do evento Y = 1, √© a probabilidade de ocorr√™ncia de churn.\nOs betas B0, B1, ‚Ä¶ Bn representam os par√¢metros do modelo, que pretendemos obter ao construir o modelo matem√°tico.\n\n\n\nAvalia√ß√£o do Modelo\n\nO teste de Kolmogorov-Smirnov (KS) pode ser utilizado como uma m√©trica para avaliar a qualidade de modelos de classifica√ß√£o, especialmente em problemas de classifica√ß√£o bin√°ria. Nesse contexto, o KS √© frequentemente empregado para avaliar a capacidade do modelo em distinguir entre as classes positiva e negativa.\n\n\nA abordagem mais comum envolve a gera√ß√£o de pontua√ß√µes (scores) ou probabilidades de predi√ß√£o para as inst√¢ncias de ambas as classes pelo modelo. Em seguida, o teste de Kolmogorov-Smirnov √© aplicado √†s distribui√ß√µes cumulativas dessas pontua√ß√µes para as duas classes. O objetivo √© verificar se h√° uma diferen√ßa significativa entre as distribui√ß√µes cumulativas das classes positiva e negativa.\n\n\nQuanto maior for a diferen√ßa entre as distribui√ß√µes cumulativas, maior ser√° o valor de KS e, consequentemente, mais eficiente ser√° o modelo, ou seja, mais capaz de classificar corretamente o que √© churn e o que n√£o √© churn.\n\n\nObtendo o melhor modelo\n\nA partir das 10 vari√°veis preditoras dispon√≠veis, foram testadas todas as combina√ß√µes poss√≠veis, selecionando de 5 a 10 vari√°veis por vez. Dessa forma, examinamos 638 modelos de regress√£o log√≠stica, avaliando o valor de KS para cada modelo nos dados de teste. Optamos pelo modelo que apresentou o maior valor de KS e o menor n√∫mero de vari√°veis preditoras que parecem ser mais relevantes para o neg√≥cio.\n\n\nPortanto, o modelo selecionado foi o que apresentou KS = 0.36 e possui as seguintes vari√°veis preditoras:\n\n‚Äòproducts_number‚Äô\n‚Äòcountry‚Äô\n‚Äògender‚Äô\n‚Äòcredit_card‚Äô\n‚Äòactive_member‚Äô\n\n\n\nRepresenta√ß√£o gr√°fica do c√°lculo do KS nos dados de teste para o modelo selecionado.\n\n\nseed          = 100\nX             = df[['products_number', 'country', 'gender', 'credit_card', 'active_member']].copy()\ny             = df[target_variable].copy()\noriginal_cols = X.columns\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n\n# ----------------------------------- Binnings --------------------------------\ndf_train = pd.merge(X_train,\n                    y_train, \n                    how         = \"left\", \n                    left_index  = True, \n                    right_index = True,\n                    validate    = \"one_to_one\")\n\ndf_test = pd.merge(X_test,\n                   y_test, \n                   how         = \"left\", \n                   left_index  = True, \n                   right_index = True,\n                   validate    = \"one_to_one\")\n            \n\ndf_train = binning_to_model(\n    df_train,\n    list(df_train.columns[df_train.columns.isin(numerical_variables)]),\n    list(df_train.columns[df_train.columns.isin(categorical_variables)]),\n    target_variable).reset_index(drop = True)\n\ndf_test = binning_to_model(\n    df_test,\n    list(df_test.columns[df_test.columns.isin(numerical_variables)]),\n    list(df_test.columns[df_test.columns.isin(categorical_variables)]),\n    target_variable).reset_index(drop = True)\n\nX_train  = df_train.drop(target_variable, axis=1).copy()\ny_train  = df_train[target_variable].copy()\n\nX_test  = df_test.drop(target_variable, axis=1).copy()\ny_test  = df_test[target_variable].copy()\n \n# -----------------------------------------------------------------------------\n\n# One-hot Encoding\nenc = OneHotEncoder(handle_unknown='ignore', drop = 'first')\nenc.fit(X_train.astype(str))\n\ncolnames = enc.get_feature_names_out()\n\n# train\ntransformed = enc.transform(X_train.astype(str)).toarray()\ndf_cat_vars = pd.DataFrame(columns=colnames, data=transformed)\nX_train = pd.concat([X_train, df_cat_vars], axis=1)\n\n# test\ntransformed = enc.transform(X_test.astype(str)).toarray()\ndf_cat_vars = pd.DataFrame(columns=colnames, data=transformed)\nX_test = pd.concat([X_test, df_cat_vars], axis=1)\n\n# Remocao das vari√°veis categoricas sem codifica√ß√£o\nX_train.drop(original_cols, axis=1, inplace = True)\nX_test.drop(original_cols, axis=1, inplace = True)\n\n# Treino\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n\n#nos_train = create_ks_table_for_logistic_regression(clf, X_train, y_train)\nnos_test  = create_ks_table_for_logistic_regression(clf, X_test, y_test)\n\n\ny_pred = np.round(clf.predict_proba(X_test)[:, 1], 2)\ny_pred = np.where(y_pred &gt;= 0.19, 1, 0)\n \ny_test = np.where(y_test == 1, \"Churn\", \"N√£o-Churn\")\ny_pred = np.where(y_pred == 1, \"Churn\", \"N√£o-Churn\")\n\n\n\n\n\n\n\n\nFaixas de opera√ß√£o do modelo selecionado\n\nA tabela abaixo apresenta todas as poss√≠veis faixas de opera√ß√£o do modelo selecionado a qual foi aplicada na base de testes. Do ponto de vista t√©cnico, a faixa 8 em destaque √© considerada a faixa √≥tima, pois possui Sens. + Espec. - 1 = KS = 0. Isso significa que essa √© a faixa com maior capacidade de classifica√ß√£o correta do que √© Churn e N√£o-Churn.\n\n\nInterpretando a tabela para essa faixa, temos que:\nProb = 19%: Significa que, se a probabilidade de ocorr√™ncia do churn for maior ou igual a 19%, o modelo classificar√° como churn.\n% Total acumulado = 40%: Significa que, dos clientes que o algoritmo avaliar, ele classificar√° 40% como churn.\nTx. classificados como 1 corretamente = 35%: Significa que, dos clientes que o algoritmo classificar como churn, 35% realmente realizariam churn, enquanto os outros 65% n√£o realizariam.\nTx. classificados como 0 corretamente = 89%: Significa que, dos clientes que o algoritmo classificar como N√£o-Churn, 89% realmente n√£o realizariam churn, enquanto os outros 11% realizariam.\nSens. = 68%: √â a sensibilidade. Significa que, de todos os casos de churn, o modelo ser√° capaz de classificar corretamente 68% deles, enquanto os outros 32% ser√£o classificados como N√£o-Churn.\nEspec. = 67%: √â a especificidade. Significa que, de todos os casos de N√£o-churn, o modelo ser√° capaz de classificar corretamente 67% deles, enquanto os outros 33% ser√£o classificados como Churn.\nSens. + Espec. - 1 = 0.36: Utilizado para calcular o valor aproximado de KS e comparar com o KS obtido no gr√°fico acima.\nAcur√°cia: De todos os clientes avaliados pelo algoritmo, o percentual de acertos, incluindo churn e n√£o churn, √© de 67%.\n\n\nnos_test.drop([\"Evento acumulado\", \"Nao-evento acumulado\", \"Ganho 1's\",\"Ganho 0's\"], axis = 1, inplace = True)\n\nCOLOR           = \"#001820\"\ndf              = nos_test\ncols_to_percent = [\n  \"Prob\",\n  \"% Total acumulado\",\n  \"Tx. classificados como 1 corretamente\",\n  \"Tx. classificados como 0 corretamente\", \n  \"Sens.\", \n  \"Espec.\",\n  \"Acur√°cia\", \n  ]\n\ncols_to_float = [\"Sens. + Espec. - 1\"]\n\ndf_styled = (\n    df.style\n        # Cor do header e index\n        .set_table_styles([{\n            'selector': 'th:not(.index_name)',\n            'props': f'background-color: {COLOR}; color: white; text-align: center;'\n        }]) \n        .set_properties(\n          **{'text-align': \"center\"})\n        .set_properties(\n            subset = ([7],) ,\n            **{'background-color': \"#C5D9F1\",\n                'color'          : \"black\",\n                'font-weight'    : \"bold\",\n                'text-align'     : \"center\"\n              })\n        .format('{:.0%}', subset= cols_to_percent) \n        .format('{:.2f}', subset= cols_to_float) \n        .hide(axis=\"index\")\n    )\ndf_styled\n\n\n\n\n\n\nFaixa\nProb\n% Total acumulado\nTx. classificados como 1 corretamente\nTx. classificados como 0 corretamente\nSens.\nEspec.\nSens. + Espec. - 1\nAcur√°cia\n\n\n\n\n1\n44%\n5%\n49%\n81%\n12%\n97%\n0.09\n79%\n\n\n2\n38%\n10%\n47%\n82%\n23%\n93%\n0.16\n79%\n\n\n3\n34%\n16%\n42%\n84%\n34%\n88%\n0.22\n77%\n\n\n4\n34%\n20%\n43%\n85%\n43%\n86%\n0.28\n77%\n\n\n5\n26%\n26%\n39%\n86%\n51%\n80%\n0.31\n74%\n\n\n6\n23%\n31%\n37%\n87%\n56%\n75%\n0.31\n71%\n\n\n7\n23%\n36%\n36%\n88%\n63%\n72%\n0.34\n70%\n\n\n8\n19%\n40%\n35%\n89%\n68%\n67%\n0.36\n67%\n\n\n9\n19%\n47%\n33%\n90%\n75%\n60%\n0.35\n63%\n\n\n10\n17%\n51%\n31%\n90%\n77%\n56%\n0.33\n60%\n\n\n11\n14%\n56%\n30%\n91%\n81%\n51%\n0.31\n57%\n\n\n12\n13%\n60%\n28%\n91%\n82%\n46%\n0.28\n53%\n\n\n13\n12%\n65%\n27%\n91%\n84%\n40%\n0.24\n49%\n\n\n14\n12%\n70%\n25%\n91%\n87%\n34%\n0.21\n45%\n\n\n15\n10%\n80%\n23%\n90%\n90%\n23%\n0.13\n37%\n\n\n16\n10%\n81%\n23%\n91%\n91%\n22%\n0.13\n36%\n\n\n17\n8%\n89%\n22%\n92%\n96%\n13%\n0.08\n30%\n\n\n18\n6%\n90%\n22%\n92%\n96%\n11%\n0.07\n29%\n\n\n19\n5%\n100%\n21%\n0%\n100%\n0%\n0.00\n21%\n\n\n\n\n\nConclus√µes Sobre o Modelo\nMas afinal, esse modelo √© bom ou ruim? Para responder a essa pergunta, devemos comparar duas situa√ß√µes: quando o modelo √© utilizado e quando n√£o √©.\nSem o modelo:\n\nSem o modelo, n√£o temos nenhuma outra forma de identificar quais clientes estariam mais propensos ao churn e tentar alguma abordagem para resgatar a confian√ßa deles, evitando sua fuga.\nIsso ocorrendo, o banco teria uma redu√ß√£o de cerca de 20% de seus clientes.\n\nCom o modelo:\n\n68% dos casos poss√≠veis de churn seriam identificados previamente, enquanto os demais 32% n√£o seriam identificados.\nSupondo que um plano de a√ß√£o seja capaz de impedir o churn de todos os poss√≠veis churn identificados (os 68%), o banco teria uma redu√ß√£o em sua carteira de clientes de apenas 6,4% (32% dos churns, os quais n√£o foram identificados pelo modelo).\nObviamente, os erros do modelo em classificar N√£o-churn como churn tamb√©m geram custos para o banco, pois ele gastar√° dinheiro para resgatar um cliente que, de fato, n√£o corria risco de perda. Isso ocorrer√° em 65% dos casos em que o modelo classificar como churn. De fato, 40% de todos os clientes ser√£o classificados como churn pelo modelo, sendo que apenas 35% desse p√∫blico realizaria o churn. A quest√£o √© que o modelo adota uma postura muito conservadora, onde a perda do cliente √© considerada algo muito mais severo do que o gasto com programas para evitar o churn. Por isso, se a probabilidade de ocorr√™ncia do churn for maior ou superior a 19%, o modelo prefere recomendar uma a√ß√£o do banco para tentar evitar a fuga desse cliente.\nDiante do exposto anteriormente, podemos afirmar que o modelo tem uma boa performance. No entanto, a decis√£o final de utiliz√°-lo ou n√£o viria ap√≥s uma an√°lise de ganho financeiro, comparando o seu uso com a n√£o utiliza√ß√£o."
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "",
    "text": "Devido aos avan√ßos da IA Generativa com os LLMs, o ChatGPT, desenvolvido pela OpenAI, tornou-se uma ferramenta popular para gera√ß√£o de texto. Atrav√©s da interface em https://chat.openai.com/, os usu√°rios podem interagir com o ChatGPT para receber respostas, resumos de texto e tradu√ß√µes. No entanto, para responder a perguntas espec√≠ficas de contexto in√©dito, como as baseadas em um documento PDF, √© geralmente necess√°rio usar a API do ChatGPT.\n\n\nPara simplificar essa tarefa, surgiu o LangChain https://www.langchain.com/, uma estrutura de c√≥digo aberto que facilita o desenvolvimento de aplicativos com LLMs. O LangChain atua como uma interface gen√©rica para diferentes LLMs, permitindo a constru√ß√£o de aplicativos LLM integrados a fontes de dados externas.\n\n\nEste projeto visa demonstrar a aplica√ß√£o pr√°tica do ChatGPT em um contexto espec√≠fico com o aux√≠lio do LangChain, explorando cada etapa e os principais desafios envolvidos."
  },
  {
    "objectID": "project1.html#setup-and-constants",
    "href": "project1.html#setup-and-constants",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Setup and Constants",
    "text": "Setup and Constants\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n# Setup\nfrom langchain.text_splitter             import RecursiveCharacterTextSplitter # Split - Tokenization\nfrom langchain.embeddings                import OpenAIEmbeddings               # Embeddings\nfrom langchain.vectorstores              import Chroma                         # Vector Store\nfrom langchain.llms                      import OpenAI                         # Models\nfrom langchain.chat_models               import ChatOpenAI                     # Chat\nfrom langchain.chains.question_answering import load_qa_chain                  # QA\nfrom langchain.callbacks                 import get_openai_callback            # Callback\nimport os\nimport textract\nimport warnings\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None) \n\nDefinindo as constantes que ser√£o utilizadas ao longo do c√≥digo.\n\n# Constants\nOPENAI_API_KEY       = os.environ.get('OPENAI_API_KEY')        # Definida como vari√°vel de ambiente\nFILE_PATH_CONTEXT    = \"./source/project1/context/cvPedro.pdf\" # Path para o arquivo de contexto\nEMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"                # Modelo para o embedding do contexto\nFILE_PATH_DB         = \"./source/project1/chroma/\"             # Path para Vector Store\nMODEL_NAME           = \"gpt-3.5-turbo\"                         # Modelo para responder as perguntas"
  },
  {
    "objectID": "project1.html#document-loading",
    "href": "project1.html#document-loading",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Document Loading",
    "text": "Document Loading\nAqui carregamos o arquivo de contexto, que pode estar em qualquer formato (https://textract.readthedocs.io/en/stable/) e convertemos em uma string.\n\n# Document Loading\nfile_path = FILE_PATH_CONTEXT\ndoc       = textract.process(file_path)\ntext      = doc.decode('utf-8')"
  },
  {
    "objectID": "project1.html#document-splitting---tokenization",
    "href": "project1.html#document-splitting---tokenization",
    "title": "Projeto: Aplica√ß√£o de LLM - ChatGPT em Contexto Espec√≠fico",
    "section": "Document Splitting - Tokenization",
    "text": "Document Splitting - Tokenization\nNesta etapa, nosso objetivo √© segmentar o contexto em v√°rios documentos, com a principal ideia de dividir o texto em unidades semanticamente relevantes.\nPor simplicidade, essa divis√£o ser√° feita considerando um n√∫mero m√°ximo de caracteres por documento, levando em conta a estrutura textual original, ou seja, espa√ßos em branco, quebras de linhas e par√°grafos podem ser considerados como separadores aqui.\nExistem diversas t√©cnicas para efetuar este processo, estamos usando apenas uma delas.\n\n# Document Splitting - Tokenization\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 512, # Quantidade m√°xima de caracteres por split\n    chunk_overlap   = 24,  # Quantidade de m√°xima de caracteres sobrepostos por split\n)\n\nchunks = text_splitter.create_documents([text])"
  },
  {
    "objectID": "project7.html",
    "href": "project7.html",
    "title": "Naive Bayes em Python e C++",
    "section": "",
    "text": "O objetivo deste projeto √© demonstrar a aplica√ß√£o do algoritmo Naive Bayes sem o uso de pacotes, ou seja, implementando-o manualmente. Realizaremos isso tanto em Python quanto em C++.\n\n\nAplicaremos o algoritmo Naive Bayes para resolver um problema de classifica√ß√£o bin√°ria, sendo que a nossa base de dados cont√©m 1046 registros e √© composta pelas seguintes vari√°veis:\n\ntipo_doc: O tipo do documento.\n\nvari√°vel discreta que pode assumir somente os seguintes valores -&gt; 1,2 ou 3.\n\nuso_dias: Tempo de uso do documento em dias.\n\nvari√°vel cont√≠nua.\n\ncertificado_valido: Se o certificado √© v√°lido ou n√£o.\n\nvari√°vel discreta que pode assumir somente os seguintes valores -&gt; 0 ou 1.\n\nclasse: Se o documento tem assinatura digital ou n√£o (vari√°vel alvo).\n\nvari√°vel bin√°ria que podese assumir os seguintes valores -&gt; 1: tem | 0: n√£o tem.\n\n\nNeste projeto nosso trabalho √© construir um modelo de Machine Learning capaz de classificar um documento como tendo ou n√£o assinatura digital com base em algumas caracter√≠sticas do documento."
  },
  {
    "objectID": "project7.html#importando-todos-os-m√≥dulos-necess√°rios",
    "href": "project7.html#importando-todos-os-m√≥dulos-necess√°rios",
    "title": "Naive Bayes em Python e C++",
    "section": "Importando todos os m√≥dulos necess√°rios",
    "text": "Importando todos os m√≥dulos necess√°rios\n\n# Setup\nfrom sklearn.metrics import confusion_matrix\nfrom typing          import Tuple\n\nimport pandas      as pd\nimport numpy       as np\nimport scipy.stats as st"
  },
  {
    "objectID": "project7.html#lendo-os-dados-a-partir-de-um-arquivo-csv",
    "href": "project7.html#lendo-os-dados-a-partir-de-um-arquivo-csv",
    "title": "Naive Bayes em Python e C++",
    "section": "Lendo os dados a partir de um arquivo csv",
    "text": "Lendo os dados a partir de um arquivo csv\n\ndf = pd.read_csv(\"source/project7/dataset.csv\")"
  },
  {
    "objectID": "project7.html#preprocessamento-dos-dados",
    "href": "project7.html#preprocessamento-dos-dados",
    "title": "Naive Bayes em Python e C++",
    "section": "Preprocessamento dos dados",
    "text": "Preprocessamento dos dados\n\n# Adicionando um nome √† primeira coluna.\ndf.rename(columns = {\"Unnamed: 0\" : \"id\"}, inplace = True)"
  },
  {
    "objectID": "project7.html#divis√£o-em-treino-e-teste",
    "href": "project7.html#divis√£o-em-treino-e-teste",
    "title": "Naive Bayes em Python e C++",
    "section": "Divis√£o em treino e teste",
    "text": "Divis√£o em treino e teste\n\n# Definido o √≠ndice que vai separar o dataset em treino e teste\nstarTest = 900\n\n# Dividindo os dados\ndf_train = df.loc[0: starTest - 1,]\ndf_test  = df.loc[starTest:,]\n\ny_train = df_train.pop(\"classe\")\nX_train = df_train.drop(\"id\", axis = 1)\n\ny_test = df_test.pop(\"classe\")\nX_test = df_test.drop(\"id\", axis = 1)"
  },
  {
    "objectID": "project7.html#cria√ß√£o-do-modelo",
    "href": "project7.html#cria√ß√£o-do-modelo",
    "title": "Naive Bayes em Python e C++",
    "section": "Cria√ß√£o do modelo",
    "text": "Cria√ß√£o do modelo\n\n# P(y|X)     = P(x1|y) * P(x2|y) * ... P(xn |y) * P(y)   / [P(x1) * P(x2) *... P(xn)]\n# posteriori = verossimilhan√ßa                  * priori / marginal\n\n# Features discretas\n# --&gt; tipo_doc\n# --&gt; certificado_valido\ndiscrete_vars   = [\"tipo_doc\", \"certificado_valido\"]\n\ndiscrete_probability_distribution = {}\n[\n discrete_probability_distribution\n .update({\n     var: pd.crosstab(y_train, X_train[var], normalize=\"index\")\n })\n for var in discrete_vars\n]\n\n\ndef discrete_likelihood(x : int, tab : pd.DataFrame) -&gt; Tuple[float, float]:\n    return tab.loc[0, x], tab.loc[1, x]\n\n# Feature cont√≠nua\n# --&gt; uso_dias\ndef continuous_likelihood(x : int, X : pd.DataFrame, y : pd.Series) -&gt; Tuple[float, float]:\n    \n    mean_0 = X.loc[y == 0,].mean()\n    std_0  = X.loc[y == 0,].std(ddof = 1)\n    pdf_0  = st.norm.pdf(x = x, loc = mean_0, scale = std_0)\n    \n    mean_1 = X.loc[y == 1,].mean()\n    std_1  = X.loc[y == 1,].std(ddof = 1)\n    pdf_1  = st.norm.pdf(x = x, loc = mean_1, scale = std_1)\n\n    return pdf_0, pdf_1"
  },
  {
    "objectID": "project7.html#aplica√ß√£o-nos-dados-de-teste",
    "href": "project7.html#aplica√ß√£o-nos-dados-de-teste",
    "title": "Naive Bayes em Python e C++",
    "section": "Aplica√ß√£o nos dados de teste",
    "text": "Aplica√ß√£o nos dados de teste\n\ndef apply(x:dict) -&gt; int:\n    \n    p_tipo_doc_y_0, p_tipo_doc_y_1 = discrete_likelihood(\n        x[\"tipo_doc\"], \n        discrete_probability_distribution[\"tipo_doc\"]\n    )\n    \n    p_certificado_valido_y_0, p_certificado_valido_y_1 = discrete_likelihood(\n        x[\"certificado_valido\"], \n        discrete_probability_distribution[\"certificado_valido\"]\n    )\n    \n    p_uso_dias_y_0, p_uso_dias_y_1 = continuous_likelihood(\n        x[\"uso_dias\"],\n        X_train[\"uso_dias\"],\n        y_train\n    )\n    \n    priori_0 = (y_train == 0).sum()/y_train.shape[0]\n    priori_1 = (y_train == 1).sum()/y_train.shape[0]\n    \n    likelihood_0 = p_tipo_doc_y_0 * p_certificado_valido_y_0 * p_uso_dias_y_0\n    likelihood_1 = p_tipo_doc_y_1 * p_certificado_valido_y_1 * p_uso_dias_y_1\n    \n    posterior_0 = likelihood_0 * priori_0\n    posterior_1 = likelihood_1 * priori_1\n    \n    return np.argmax(np.array([posterior_0, posterior_1]))   \n\n\ny_pred_train = []\nfor x in X_train.to_dict(orient = \"records\"):\n    y_pred_train.append(apply(x))\n    \ny_pred_test = []\nfor x in X_test.to_dict(orient = \"records\"):\n    y_pred_test.append(apply(x))"
  },
  {
    "objectID": "project7.html#avalia√ß√£o-do-modelo-nos-dados-de-teste",
    "href": "project7.html#avalia√ß√£o-do-modelo-nos-dados-de-teste",
    "title": "Naive Bayes em Python e C++",
    "section": "Avalia√ß√£o do modelo nos dados de teste",
    "text": "Avalia√ß√£o do modelo nos dados de teste\n\n# Matriz de confus√£o\ncm  = confusion_matrix(y_test, y_pred_test)\n\n# Acur√°cia\ndef acc(cm : np.ndarray) -&gt; float:\n    TN = cm[0,0]\n    FP = cm[0,1]\n    FN = cm[1,0]\n    TP = cm[1,1]\n    \n    acc_val = (TN + TP)/(TN + FP + FN + TP)\n    return acc_val\n\n\n# Sensibilidade\ndef sens(cm : np.ndarray) -&gt; float:\n    FN = cm[1,0]\n    TP = cm[1,1]\n    \n    sens_val = TP/(TP + FN)\n    return sens_val\n\n# Especificidade\ndef spec(cm : np.ndarray) -&gt; float:\n    TN = cm[0,0]\n    FP = cm[0,1]\n    \n    spec_val = TN/(TN + FP)\n    return spec_val\n\n\nprint(f\"Acur√°cia: {acc(cm)}\")\nprint(f\"Sensibilidade {sens(cm)}\")\nprint(f\"Especificidade {spec(cm)}\")\n\nAcur√°cia: 0.7602739726027398\nSensibilidade 0.6268656716417911\nEspecificidade 0.8734177215189873"
  },
  {
    "objectID": "project7.html#libs-e-namespace",
    "href": "project7.html#libs-e-namespace",
    "title": "Naive Bayes em Python e C++",
    "section": "Libs e Namespace",
    "text": "Libs e Namespace\n\n// Supressao de warnings\n#pragma GCC diagnostic push                                             \n#pragma GCC diagnostic ignored \"-Wsign-compare\"\n\n// Libs\n#include &lt;iostream&gt;  // Entrada/Sa√≠da de dados\n#include &lt;fstream&gt;   // Manipula√ß√£o de arquivos\n#include &lt;vector&gt;    // Vetores\n#include &lt;string&gt;    // trabalhar com strings\n#include &lt;numeric&gt;   // trabalhar com numeros\n#include &lt;algorithm&gt; // Preprocessamento de dados\n#include &lt;cmath&gt;     // matem√°tica\n#include &lt;math.h&gt;    // matematica\n\n// Namespace\nusing namespace std;"
  },
  {
    "objectID": "project7.html#constantes-e-estruturas-de-dados",
    "href": "project7.html#constantes-e-estruturas-de-dados",
    "title": "Naive Bayes em Python e C++",
    "section": "Constantes e Estruturas de dados",
    "text": "Constantes e Estruturas de dados\n// √åndice de in√≠cio dos dados de teste\nconst int starTest = 900;\n\n// Struct\nstruct DataFrame {\n\n    vector&lt;string&gt; cols;\n    vector&lt;vector&lt;double&gt;&gt; data;\n\n    DataFrame(int nrow, int ncols):\n        cols(ncols, \"NA\"),\n        data(nrow, vector&lt;double&gt; (ncols, 0)) {}\n};"
  },
  {
    "objectID": "project7.html#prototipa√ß√£o-das-fun√ß√µes-criadas",
    "href": "project7.html#prototipa√ß√£o-das-fun√ß√µes-criadas",
    "title": "Naive Bayes em Python e C++",
    "section": "Prototipa√ß√£o das fun√ß√µes criadas",
    "text": "Prototipa√ß√£o das fun√ß√µes criadas\n// Leitura do arquivo .csv\nifstream read_file(string file_path);\n\n// Instancia um objeto do tipo DataFrame\nDataFrame create_DataFrame(vector&lt;string&gt; cols, vector&lt;vector&lt;double&gt;&gt; data);\n\n// Printa um dataframe\nvoid print_DataFrame(DataFrame df, int n, string where = \"top\");\n\n// Reliza o slice em vetorores 1D\nvector&lt;double&gt; slice(vector&lt;double&gt; v, int start, int end);\n\n// Calcula uma tabela de frequ√™ncias para a feature certificado_valido\nvector&lt;vector&lt;double&gt;&gt; get_distribution_certificado_valido(DataFrame df, vector&lt;vector&lt;double&gt;&gt; tab);\n\n// Calcula uma tabela de frequ√™ncias para a feature tipo_doc\nvector&lt;vector&lt;double&gt;&gt; get_distribution_tipo_doc(DataFrame df, vector&lt;vector&lt;double&gt;&gt; tab);\n\n// Calcula a m√©dia para a v√°ri√°vel target classe quando y = 0 e quando y = 1\nvector&lt;double&gt;  calc_mean(DataFrame df);\n\n// Calcula o desvio padr√£o para a v√°ri√°vel target classe quando y = 0 e quando y = 1\nvector&lt;double&gt; calc_std(DataFrame df, vector&lt;double&gt; mean);\n\n// Encapsula a execu√ß√£o das fun√ß√µes \n// --&gt; get_distribution_certificado_valido\n// --&gt; get_distribution_tipo_doc\n// --&gt; calc_mean\n// --&gt; calc_std\n// Gerando um vetor de vetores do tipo double que representa o modelo treinado\nvector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; fit(DataFrame df);\n\n// Calcula as verossimilhan√ßas P(certificado_valido = x | classe = y)\ndouble likelihood_certificado_valido(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x);\n\n// Calcula as verossimilhan√ßas P(tipo_doc = x | classe = y)\ndouble likelihood_tipo_doc(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x);\n\n// Calcula as verossimilhan√ßas P(uso_dias = x | classe = y)\ndouble likelihood_uso_dias(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x);\n\n// Calcula as probabilidades √† priori P(classe = 0) e P(classe = 1) \nvector&lt;double&gt; priori(DataFrame df);\n\n// Aplica o modelo na base de testes\nvector&lt;double&gt; apply_model(DataFrame df_test, DataFrame df_train, vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs);\n\n// Avalia a performance do modelo obtido\nvoid evaluate(vector&lt;double&gt; y_true, vector&lt;double&gt; y_pred);"
  },
  {
    "objectID": "project7.html#main",
    "href": "project7.html#main",
    "title": "Naive Bayes em Python e C++",
    "section": "Main",
    "text": "Main\nint main() {\n\n    cout &lt;&lt; endl &lt;&lt;\"#################### Naive Bayes #########################\" &lt;&lt; endl &lt;&lt; endl;\n\n    //------------------------------Leitura da base de dados------------------------------------\n    ifstream inputFile;\n    inputFile = read_file(\"src/dataset.csv\");\n\n    //-------------------------------Declara√ß√£o das Vars ---------------------------------------\n    // Vari√°veis escalares do tipo double para tratar os valores de cada coluna\n    double idVal;\n    double tipo_docVal;\n    double classeVal;\n    double certificado_validoVal;\n    double uso_diasVal;\n\n    // Vari√°veis do tipo vetor para todos os elementos de cada coluna do dataset\n    vector&lt;double&gt; id;\n    vector&lt;double&gt; tipo_doc;            \n    vector&lt;double&gt; classe;      \n    vector&lt;double&gt; certificado_valido;      \n    vector&lt;double&gt; uso_dias;\n\n    // Vari√°vel para armazenar o cabe√ßalho do arquivo\n    string header;\n\n    // Vari√°vel para armazenar cada c√©lula do arquivo csv\n    string cell;\n\n    // Pegar a primeira linha do aqruivo csv\n    getline(inputFile, header);\n\n    //-----------------------------Loop de Carga e Limpeza dos Dados-----------------------------\n    // Loop de carga e limpeza inicial dos dados\n    while(inputFile.good()) {\n                \n        // Leitura do primeiro elemento da linha (coluna id)\n        getline(inputFile, cell, ',');          \n\n        // Remove aspas     \n        cell.erase(remove(cell.begin(), cell.end(), '\\\"' ),cell.end()); \n    \n        // Agora seguimos fazendo a leitura somente das c√©lulas com valores\n        if(!cell.empty()) {                             \n\n            idVal = stod(cell);   // str to double  \n            id.push_back(idVal);  // Append do valor de x no vetor                  \n                            \n            // Leitura da coluna tipo_doc\n            getline(inputFile, cell, ',');\n            tipo_docVal = stod(cell);               \n            tipo_doc.push_back(tipo_docVal);        \n                \n            // Leitura da coluna classe \n            getline(inputFile, cell, ',');      \n            classeVal = stod(cell);     \n            classe.push_back(classeVal);    \n            \n            // Leitura da coluna certificado_valido \n            getline(inputFile, cell, ',');      \n            certificado_validoVal = stod(cell);             \n            certificado_valido.push_back(certificado_validoVal);                \n                \n            // Leitura da coluna uso_dias\n            getline(inputFile, cell);                       \n            uso_diasVal = stod(cell);               \n            uso_dias.push_back(uso_diasVal);                    \n        }\n        else {\n\n            // Se linha vazia, finaliza o loop                                          \n            break;\n        }   \n    }\n\n    //--------------------------------Cria√ß√£o dos DataFrames--------------------------------\n    vector&lt;string&gt; cols = {\"id\",\"certificado_valido\", \"classe\", \"tipo_doc\", \"uso_dias\"};\n    vector&lt;vector&lt;double&gt;&gt; data = {id, certificado_valido, classe, tipo_doc, uso_dias};\n    DataFrame df(id.size(), cols.size());\n\n    df = create_DataFrame(cols, data);\n    \n    cout &lt;&lt; \"#------------Dataframe - primeiras 5 linhas------------#\"  &lt;&lt; endl;\n    print_DataFrame(df, 5);\n    cout &lt;&lt; endl;\n    //cout &lt;&lt; \"#-------------Dataframe - √∫ltimas 5 linhas-------------#\"  &lt;&lt; endl;\n    //print_DataFrame(df, 5, \"bottom\");\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    //Treino\n    vector&lt;vector&lt;double&gt;&gt; data_train;\n    DataFrame df_train(starTest, cols.size());\n    data_train = {\n        slice(id,                 0, starTest), \n        slice(certificado_valido, 0, starTest), \n        slice(classe,             0, starTest), \n        slice(tipo_doc,           0, starTest), \n        slice(uso_dias,           0, starTest)\n    };\n\n    df_train = create_DataFrame(cols, data_train);\n    cout &lt;&lt; \"#---------Dataframe Treino - primeiras 5 linhas---------#\" &lt;&lt; endl;\n    print_DataFrame(df_train, 5);\n    cout &lt;&lt; endl;\n    //cout &lt;&lt; \"#----------Dataframe Treino - √∫ltimas 5 linhas----------#\"&lt;&lt; endl;\n    //print_DataFrame(df_train, 5, \"bottom\");\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    //Teste\n    vector&lt;vector&lt;double&gt;&gt; data_test;\n    DataFrame df_test(id.size() - starTest, cols.size());\n    data_test = {\n        slice(id,                 starTest, id.size()), \n        slice(certificado_valido, starTest, id.size()), \n        slice(classe,             starTest, id.size()), \n        slice(tipo_doc,           starTest, id.size()), \n        slice(uso_dias,           starTest, id.size())\n    };\n\n    df_test = create_DataFrame(cols, data_test);                    \n    cout &lt;&lt; \"#---------Dataframe Teste - primeiras 5 linhas---------#\" &lt;&lt; endl;\n    print_DataFrame(df_test, 5);\n    cout &lt;&lt; endl;\n    //cout &lt;&lt; \"#---------Dataframe Teste - √∫ltimas 5 linhas----------#\"  &lt;&lt; endl;\n    //print_DataFrame(df_test, 5, \"bottom\");\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    //------------------------------Treina Modelo---------------------------------------\n    vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs;\n    probs = fit(df_train);\n\n    //------------------------------Aplica Modelo---------------------------------------\n    vector&lt;double&gt; y_pred = apply_model(df_test, df_train, probs);\n\n    //------------------------------Avalia√ß√£o-------------------------------------------\n    vector&lt;double&gt; y_true;\n    for(auto row: df_test.data) {y_true.push_back(row.at(2));};\n    evaluate(y_true, y_pred);\n\n    cout &lt;&lt; endl &lt;&lt;\"##########################################################\" &lt;&lt; endl;\n};"
  },
  {
    "objectID": "project7.html#fun√ß√µes",
    "href": "project7.html#fun√ß√µes",
    "title": "Naive Bayes em Python e C++",
    "section": "Fun√ß√µes",
    "text": "Fun√ß√µes\n\nread_file\nifstream read_file(string file_path) {\n\n    ifstream inputFile;\n    inputFile.open(file_path);\n\n    // Verifica se h√° algum erro\n    if(!inputFile.is_open()) {\n        cout &lt;&lt; \"Falha ao abrir o arquivo\" &lt;&lt; endl;\n    }\n\n    return inputFile;\n};\n\n\ncreate_DataFrame\nDataFrame create_DataFrame(vector&lt;string&gt; cols, vector&lt;vector&lt;double&gt;&gt; data) {\n\n    int nrows, ncols;\n    nrows = data.at(0).size();\n    ncols = cols.size();\n    DataFrame df(nrows, ncols);\n\n    for (int i = 0; i &lt; ncols; i++) {\n        df.cols.at(i) = cols.at(i);  \n    }\n\n    for (int i = 0; i &lt; ncols; i++) {\n        for (int j = 0; j &lt; nrows; j++) {\n            df.data.at(j).at(i) = data.at(i).at(j);\n        }\n        \n    }\n\n    return df;\n};\n\n\nprint_DataFrame\nvoid print_DataFrame(DataFrame df, int n, string where) {\n\n    // header\n    for(auto var: df.cols) {\n        cout &lt;&lt; var &lt;&lt; \"   \";\n    }\n    cout &lt;&lt; endl;\n\n\n    if(where == \"top\") {\n        // row\n        for (int i = 0; i &lt; n; i++) {\n            cout &lt;&lt; \n                df.data.at(i).at(0) &lt;&lt; \"             \" &lt;&lt;\n                df.data.at(i).at(1) &lt;&lt; \"            \" &lt;&lt;\n                df.data.at(i).at(2) &lt;&lt; \"        \" &lt;&lt;\n                df.data.at(i).at(3) &lt;&lt; \"         \" &lt;&lt;\n                df.data.at(i).at(4) &lt;&lt; endl;\n            \n        }\n\n    }\n\n    if(where == \"bottom\") {\n        int start = df.data.size() - n;\n        // row\n        for (int i = start; i &lt; df.data.size(); i++) {\n            cout &lt;&lt; \n                df.data.at(i).at(0) &lt;&lt; \"             \" &lt;&lt;\n                df.data.at(i).at(1) &lt;&lt; \"            \" &lt;&lt;\n                df.data.at(i).at(2) &lt;&lt; \"        \" &lt;&lt;\n                df.data.at(i).at(3) &lt;&lt; \"         \" &lt;&lt;\n                df.data.at(i).at(4) &lt;&lt; endl;\n            \n        }\n    }\n\n};\n\n\nslice\nvector&lt;double&gt; slice(vector&lt;double&gt; v, int start, int end) {\n    // Verifica se os √≠ndices s√£o v√°lidos\n    if (start &lt; 0 || end &gt; v.size() || start &gt; end) {\n        throw out_of_range(\"Invalid slice indices\");\n    }\n    return vector&lt;double&gt;(v.begin() + start, v.begin() + end);\n}\n\n\nget_distribution_certificado_valido\nvector&lt;vector&lt;double&gt;&gt; get_distribution_certificado_valido(DataFrame df, vector&lt;vector&lt;double&gt;&gt; tab) {\n\n    for (auto row: df.data) {\n\n        if(row.at(1) == 0 && row.at(2) == 0) {\n            tab.at(0).at(0)++;\n        }\n\n        if(row.at(1) == 1 && row.at(2) == 0) {\n            tab.at(0).at(1)++;\n        }\n\n        if(row.at(1) == 0 && row.at(2) == 1) {\n            tab.at(1).at(0)++;\n        }\n\n        if(row.at(1) == 1 && row.at(2) == 1) {\n            tab.at(1).at(1)++;\n        }\n\n    }\n\n    cout &lt;&lt; \"Distribui√ß√£o da feature certificado_valido\" &lt;&lt; endl;\n    tab.at(0).at(0) = tab.at(0).at(0) / (tab.at(0).at(0) + tab.at(0).at(1)); \n    tab.at(0).at(1) = 1 - tab.at(0).at(0);\n    \n    tab.at(1).at(0) = tab.at(1).at(0) / (tab.at(1).at(1) + tab.at(1).at(0)); \n    tab.at(1).at(1) = 1 - tab.at(1).at(0);\n\n    cout &lt;&lt; \"        x = 0     x = 1\" &lt;&lt; endl;\n    cout &lt;&lt;\"y = 0 \"&lt;&lt;tab.at(0).at(0) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(0).at(1) &lt;&lt; endl;\n    cout &lt;&lt;\"y = 1 \"&lt;&lt;tab.at(1).at(0) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(1).at(1) &lt;&lt; endl;\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    return tab;\n}\n\n\nget_distribution_tipo_doc\nvector&lt;vector&lt;double&gt;&gt; get_distribution_tipo_doc(DataFrame df, vector&lt;vector&lt;double&gt;&gt; tab) {\n\n    for (auto row: df.data) {\n\n        if(row.at(3) == 1 && row.at(2) == 0) {\n            tab.at(0).at(0)++;\n        }\n\n        if(row.at(3) == 2 && row.at(2) == 0) {\n            tab.at(0).at(1)++;\n        }\n\n        if(row.at(3) == 3 && row.at(2) == 0) {\n            tab.at(0).at(2)++;\n        }\n\n        if(row.at(3) == 1 && row.at(2) == 1) {\n            tab.at(1).at(0)++;\n        }\n\n        if(row.at(3) == 2 && row.at(2) == 1) {\n            tab.at(1).at(1)++;\n        }\n\n        if(row.at(3) == 3 && row.at(2) == 1) {\n            tab.at(1).at(2)++;\n        }\n\n    }\n\n    cout &lt;&lt; \"Distribui√ß√£o da feature tipo_doc\" &lt;&lt; endl;\n    double sum;\n\n    sum = (tab.at(0).at(0) + tab.at(0).at(1) + tab.at(0).at(2));\n    tab.at(0).at(0) = tab.at(0).at(0) / sum;\n    tab.at(0).at(1) = tab.at(0).at(1) / sum;\n    tab.at(0).at(2) = tab.at(0).at(2) / sum; \n    \n    sum = (tab.at(1).at(0) + tab.at(1).at(1) + tab.at(1).at(2)); \n    tab.at(1).at(0) = tab.at(1).at(0) / sum;\n    tab.at(1).at(1) = tab.at(1).at(1) / sum; \n    tab.at(1).at(2) = tab.at(1).at(2) / sum; \n\n    cout &lt;&lt; \"        x = 1     x = 2    x = 3\" &lt;&lt; endl;\n    cout &lt;&lt;\"y = 0 \"&lt;&lt;tab.at(0).at(0) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(0).at(1) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(0).at(2) &lt;&lt; endl;\n    cout &lt;&lt;\"y = 1 \"&lt;&lt;tab.at(1).at(0) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(1).at(1) &lt;&lt; \" \";\n    cout &lt;&lt; tab.at(1).at(2) &lt;&lt; endl;\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    return tab;\n};\n\n\ncalc_mean\nvector&lt;double&gt; calc_mean(DataFrame df) {\n\n    vector&lt;double&gt; mean_uso_dias(2,0);\n    double sum_0, sum_1;\n    int cnt_0, cnt_1;\n\n    sum_0 = 0;\n    sum_1 = 0;\n    cnt_0 = 0;\n    cnt_1 = 0;\n\n    for(auto row: df.data) {\n        if(row.at(2) == 0) {\n            sum_0 += row.at(4);\n            cnt_0++;\n        }\n        if(row.at(2) == 1) {\n            sum_1 += row.at(4);\n            cnt_1++;\n        }\n    }\n\n    mean_uso_dias.at(0) = sum_0/cnt_0;\n    mean_uso_dias.at(1) = sum_1/cnt_1;\n\n    cout &lt;&lt; \"M√©dias da feature uso_dias\" &lt;&lt; endl;\n    cout &lt;&lt; \"y = 0     y = 1\" &lt;&lt; endl;\n    cout &lt;&lt; mean_uso_dias.at(0) &lt;&lt;\" \";\n    cout &lt;&lt; mean_uso_dias.at(1) &lt;&lt; endl;\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    return mean_uso_dias;\n};\n\n\ncalc_std\nvector&lt;double&gt; calc_std(DataFrame df, vector&lt;double&gt; mean) {\n\n    vector&lt;double&gt; std_uso_dias(2,0);\n    double sum_0, sum_1;\n    int cnt_0, cnt_1;\n\n    sum_0 = 0;\n    sum_1 = 0;\n    cnt_0 = 0;\n    cnt_1 = 0;\n\n    for(auto row: df.data) {\n        if(row.at(2) == 0) {\n            sum_0 += pow((row.at(4) - mean.at(0)), 2);\n            cnt_0++;\n        }\n        if(row.at(2) == 1) {\n            sum_1 += pow((row.at(4) - mean.at(1)), 2);\n            cnt_1++;\n        }\n    }\n\n    std_uso_dias.at(0) = sqrt(sum_0/(cnt_0 - 1));\n    std_uso_dias.at(1) = sqrt(sum_1/(cnt_1 - 1));\n\n    cout &lt;&lt; \"Desvio padr√£o da feature uso_dias\" &lt;&lt; endl;\n    cout &lt;&lt; \"y = 0     y = 1\" &lt;&lt; endl;\n    cout &lt;&lt; std_uso_dias.at(0) &lt;&lt;\" \";\n    cout &lt;&lt; std_uso_dias.at(1) &lt;&lt; endl;\n    cout &lt;&lt; endl &lt;&lt;\"=======================================================\" &lt;&lt; endl;\n\n    return std_uso_dias;\n};\n\n\nfit\nvector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; fit(DataFrame df) {\n\n    // ------------------------- Discretas ------------------------------------\n    vector&lt;vector&lt;double&gt;&gt;  probability_distribution_certificado_valido(2, vector&lt;double&gt;(2,0)); // 2 x 2\n    vector&lt;vector&lt;double&gt;&gt;  probability_distribution_tipo_doc(2, vector&lt;double&gt;(3,0));           // 2 x 3\n\n    // --&gt; certificado_valido\n    probability_distribution_certificado_valido = get_distribution_certificado_valido(\n        df, \n        probability_distribution_certificado_valido\n    );\n\n    // --&gt; tipo_doc\n    probability_distribution_tipo_doc = get_distribution_tipo_doc(\n        df, \n        probability_distribution_tipo_doc\n    );\n\n    // ------------------------- Cont√≠nuas ------------------------------------\n    vector&lt;double&gt; mean_uso_dias(2,0);                                                 // 1x2 \n    vector&lt;double&gt; std_uso_dias(2,0);                                                  // 1x2\n    vector&lt;vector&lt;double&gt;&gt;  probability_distribution_uso_dias(2, vector&lt;double&gt;(2,0)); // 2 x 2\n\n    // --&gt; uso_dias\n    mean_uso_dias                     = calc_mean(df);\n    std_uso_dias                      = calc_std(df, mean_uso_dias);\n    probability_distribution_uso_dias = {\n        mean_uso_dias,\n        std_uso_dias\n    };\n\n    // ------------------------- Resultado ------------------------------------\n        \n    // Result\n    vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs;\n    probs = {\n        probability_distribution_certificado_valido,\n        probability_distribution_tipo_doc,\n        probability_distribution_uso_dias\n    };\n\n    return probs;\n\n};\n\n\nlikelihood_certificado_valido\ndouble likelihood_certificado_valido(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x) {\n    return probs.at(0).at(y).at(val_x);\n};\n\n\nlikelihood_tipo_doc\ndouble likelihood_tipo_doc(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x) {\n    return probs.at(1).at(y).at(val_x-1);\n}\n\n\nlikelihood_uso_dias\ndouble likelihood_uso_dias(vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs, int y, double val_x) {\n\n    double mean, std, var, den, num, pdf;\n\n    mean = probs.at(2).at(0).at(y);\n    std  = probs.at(2).at(1).at(y);\n    var  = pow(std, 2);\n\n    // Gaussian\n    den = sqrt((2 * M_PI * var));\n    num = exp(-pow((val_x- mean), 2)  /(2*var));\n    pdf = num/den;\n\n    //cout &lt;&lt;\"y = \"&lt;&lt;y&lt;&lt; \"| valor =  \" &lt;&lt; val_x &lt;&lt; \"| mean = \" &lt;&lt; mean &lt;&lt; \"| std = \" &lt;&lt; std &lt;&lt; \"| pdf = \" &lt;&lt; pdf &lt;&lt; endl;\n    return pdf;\n}\n\n\npriori\nvector&lt;double&gt; priori(DataFrame df) {\n\n    vector&lt;double&gt; priori(2,0);\n    double cnt_0, cnt_1;\n\n    cnt_0 = 0;\n    cnt_1 = 0;\n\n    for(auto row: df.data) {\n        if(row.at(2) == 0) {\n            cnt_0++;\n        }\n        if(row.at(2) == 1) {\n            cnt_1++;\n        }\n    }\n\n    priori = {\n        cnt_0/df.data.size(),\n        cnt_1/df.data.size()\n    };\n\n    return priori;\n};\n\n\napply_model\nvector&lt;double&gt; apply_model(DataFrame df_test, DataFrame df_train, vector&lt;vector&lt;vector&lt;double&gt;&gt;&gt; probs) {\n\n    //posteriori   \n    double posterior_0, posterior_1;\n\n    //piori\n    vector&lt;double&gt; priori_all = priori(df_train);\n    double priori_0, priori_1;\n\n    priori_0 = priori_all.at(0);\n    priori_1 = priori_all.at(1);\n\n    //Verossimilhan√ßa\n    double p_tipo_doc_y_0, p_tipo_doc_y_1;\n    double p_certificado_valido_y_0, p_certificado_valido_y_1;\n    double p_uso_dias_y_0, p_uso_dias_y_1;\n    double likelihood_0,likelihood_1;\n    double val_x, y;\n\n    // pred\n    vector&lt;double&gt; y_pred;\n\n    // Calcula\n    for (auto row: df_test.data) {\n\n        // certificado_valido\n        val_x = row.at(1);\n        p_certificado_valido_y_0 = likelihood_certificado_valido(probs, 0, val_x);\n        p_certificado_valido_y_1 = likelihood_certificado_valido(probs, 1, val_x);\n\n        // tipo_doc\n        val_x = row.at(3);\n        p_tipo_doc_y_0 = likelihood_tipo_doc(probs, 0, val_x);\n        p_tipo_doc_y_1 = likelihood_tipo_doc(probs, 1, val_x);\n\n        // uso_dias\n        val_x = row.at(4);\n        p_uso_dias_y_0 = likelihood_uso_dias(probs, 0, val_x);\n        p_uso_dias_y_1 = likelihood_uso_dias(probs, 1, val_x);\n\n        likelihood_0 = p_certificado_valido_y_0 * p_tipo_doc_y_0 * p_uso_dias_y_0;\n        likelihood_1 = p_certificado_valido_y_1 * p_tipo_doc_y_1 * p_uso_dias_y_1;\n\n        posterior_0 = likelihood_0 * priori_0;\n        posterior_1 = likelihood_1 * priori_1;\n\n        // normaliza\n        if(posterior_0 &gt; posterior_1){\n            y_pred.push_back(0);\n        } else {\n            y_pred.push_back(1);\n        }\n    }\n    return y_pred;\n};\n\n\nevaluate\nvoid evaluate(vector&lt;double&gt; y_true, vector&lt;double&gt; y_pred){\n\n    double tp = 0, tn = 0, fp = 0, fn = 0;\n\n    for (int i = 0; i &lt; y_pred.size(); i++) {\n\n        if(y_true.at(i) == 1 && y_pred.at(i) == 1){\n            tp++;\n        }\n        if(y_true.at(i) == 1 && y_pred.at(i) == 0){\n            fn++;\n        }\n\n        if(y_true.at(i) == 0 && y_pred.at(i) == 1){\n            fp++;\n        }\n\n        if(y_true.at(i) == 0 && y_pred.at(i) == 0){\n            tn++;\n        }\n    }\n\n    double acc  = (tp + tn)/(tp + tn + fp + fn);\n    double sens = tp/(tp + fn);\n    double spec = tn/(tn + fp);\n\n    cout &lt;&lt; \"Avalia√ß√£o do Modelo\" &lt;&lt; endl;\n    cout &lt;&lt; \"Acur√°cia: \" &lt;&lt; acc   &lt;&lt; endl \n    &lt;&lt; \"Sensibilidade: \" &lt;&lt; sens  &lt;&lt; endl \n    &lt;&lt;\"Especificidade: \" &lt;&lt; spec  &lt;&lt; endl; \n}"
  },
  {
    "objectID": "project7.html#output",
    "href": "project7.html#output",
    "title": "Naive Bayes em Python e C++",
    "section": "Output",
    "text": "Output\n\n\n\n#################### Naive Bayes #########################\n\n#------------Dataframe - primeiras 5 linhas------------#\nid   certificado_valido   classe   tipo_doc   uso_dias   \n738             1            0        3         19\n868             0            1        3         22\n971             1            1        3         20\n938             0            0        3         1\n456             1            0        2         63\n\n\n=======================================================\n#---------Dataframe Treino - primeiras 5 linhas---------#\nid   certificado_valido   classe   tipo_doc   uso_dias   \n738             1            0        3         19\n868             0            1        3         22\n971             1            1        3         20\n938             0            0        3         1\n456             1            0        2         63\n\n\n=======================================================\n#---------Dataframe Teste - primeiras 5 linhas---------#\nid   certificado_valido   classe   tipo_doc   uso_dias   \n654             0            1        3         13\n870             1            0        3         28\n143             1            0        1         46\n731             1            0        3         27\n171             1            1        1         49\n\n\n=======================================================\nDistribui√ß√£o da feature certificado_valido\n        x = 0     x = 1\ny = 0 0.159259 0.840741\ny = 1 0.694444 0.305556\n\n=======================================================\nDistribui√ß√£o da feature tipo_doc\n        x = 1     x = 2    x = 3\ny = 0 0.168519 0.22037 0.611111\ny = 1 0.416667 0.263889 0.319444\n\n=======================================================\nM√©dias da feature uso_dias\ny = 0     y = 1\n30.4168 28.9206\n\n=======================================================\nDesvio padr√£o da feature uso_dias\ny = 0     y = 1\n14.2118 15.0907\n\n=======================================================\nAvalia√ß√£o do Modelo\nAcur√°cia: 0.760274\nSensibilidade: 0.626866\nEspecificidade: 0.873418\n\n##########################################################"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "",
    "text": "O mercado imobili√°rio √© uma fonte rica de informa√ß√µes sobre propriedades dispon√≠veis para venda ou aluguel. No entanto, navegar por in√∫meros an√∫ncios em diferentes plataformas pode ser uma tarefa √°rdua e demorada. √â aqui que entra o web scraping, uma t√©cnica poderosa para extrair dados de maneira automatizada e eficiente.\n\n\nNeste projeto, embarcaremos em uma jornada para extrair informa√ß√µes de im√≥veis do renomado site Viva Real https://www.vivareal.com.br/.\n\n\nUsando BeautifulSoup em Python, iremos explorar a estrutura HTML das p√°ginas do Viva Real, navegar pelos elementos relevantes e extrair detalhes importantes, como pre√ßo, localiza√ß√£o, caracter√≠sticas do im√≥vel e informa√ß√µes sobre o anunciante."
  },
  {
    "objectID": "project4.html#setup",
    "href": "project4.html#setup",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Setup:",
    "text": "Setup:\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n# Imports\nfrom   bs4 import BeautifulSoup\nimport numpy  as np\nimport pandas as pd\nimport requests\nimport time"
  },
  {
    "objectID": "project4.html#dados-que-vamos-extrair",
    "href": "project4.html#dados-que-vamos-extrair",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Dados que vamos extrair:",
    "text": "Dados que vamos extrair:\nCriando as listas onde vamos armazenar os dados raspados.\n\n# Lists\nfull_adress      = []\nfull_title       = []\nfull_area        = []\nfull_area_unit   = []\nfull_room        = []\nfull_bath        = []\nfull_garage      = []\nfull_price       = []\nfull_price_condo = []\nfull_publisher   = []"
  },
  {
    "objectID": "project4.html#scraping",
    "href": "project4.html#scraping",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Scraping",
    "text": "Scraping\n\nEstamos interessados em extrair informa√ß√µes de todos os im√≥veis dispon√≠veis para loca√ß√£o na cidade de Jaguari√∫na - SP.\n\n\nEm 03/05/2024, esses im√≥veis est√£o distribu√≠dos em 17 p√°ginas, totalizando 612.\n\n\nPara cada im√≥vel, ainda teremos que acessar uma p√°gina extra para conseguirmos coletar o nome da imobili√°ria anunciante, portanto, vamos raspar um total de 17 + 612 = 629 p√°ginas.\n\n\nDesta forma, sempre que fizermos uma requisi√ß√£o em uma p√°gina, vamos esperar 10 segundos para fazer a pr√≥xima, a fim de n√£o sobrecarregar o servidor.\n\n\nt = 10 \n\n# Pages\nfor p in range(1,18):\n    \n    print(f\"pagina: {p}\")\n    \n    time.sleep(t)   \n    \n    url     = f\"https://www.vivareal.com.br/aluguel/sp/jaguariuna?page={p}\"\n    request = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:124.0) Gecko/20100101 Firefox/124.0\"})\n    soup    = BeautifulSoup(request.text, \"html.parser\")\n    \n\n    # Imo\n    builds  = soup.findAll(class_ = \"property-card__content-link js-card-title\")\n\n    # link\n    link_builds = soup.findAll(\"a\", attrs={\"class\": \"property-card__labels-container js-main-info js-listing-labels-link\"})\n    url_link = [\"https://www.vivareal.com.br\" + link[\"href\"] for link in link_builds]\n    \n    \n    i = 0\n    for build, link in zip(builds, url_link):\n        \n        i += 1\n        print(f\"imo: {i}\")\n            \n    \n        try:\n            adress = build.find(\"span\", attrs={\"class\": \"property-card__address\"}).text\n            full_adress.append(adress.strip())\n        except:\n            full_adress.append(\"VAZIO\")\n    \n        try:\n            title = build.find(\"span\", attrs={\"class\": \"property-card__title js-cardLink js-card-title\"}).text\n            full_title.append(title.strip())\n        except:\n            full_title.append(\"VAZIO\")\n    \n        try:\n            area = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-area\"}).text.split()\n            full_area.append(area[0].strip())\n        except:\n            full_area.append(\"0\")\n    \n        try:\n            area = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-area\"}).text.split()\n            full_area_unit.append(area[1].strip())\n        except:\n            full_area_unit.append(\"0\")\n    \n        try:\n            room = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-room js-property-detail-rooms\"}).text.split()\n            full_room.append(room[0].strip())\n        except:\n            full_room.append(\"0\")\n    \n        try:\n            bath = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom\"}).text.split()\n            full_bath.append(bath[0].strip())\n        except:\n            full_bath.append(\"0\")\n    \n        try:\n            garage = build.find(\"li\", attrs={\"class\": \"property-card__detail-item property-card__detail-garage js-property-detail-garages\"}).text.split()\n            full_garage.append(garage[0].strip())\n        except:\n            full_garage.append(\"0\")\n    \n        try:\n            price = build.find(\"div\", attrs={\"class\": \"property-card__price js-property-card-prices js-property-card__price-small\"}).text\n            full_price.append(price.strip())\n        except:\n            full_price.append(\"0\")\n    \n        try:\n            price_condo = build.find(\"div\", attrs={\"class\": \"property-card__price-details--condo\"}).text.split()\n            full_price_condo.append(price_condo[-1].strip())\n        except:\n            full_price_condo.append(\"0\")\n            \n        time.sleep(t)\n        try:\n            request2   = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"})\n            soup2      = BeautifulSoup(request2.text, \"html.parser\")\n            publisher = soup2.find(\"a\", attrs={\"class\": \"publisher-details__name\"})\n            full_publisher.append( publisher.text.strip())\n        except:\n            full_publisher.append('0')"
  },
  {
    "objectID": "project4.html#salvando-o-resultado",
    "href": "project4.html#salvando-o-resultado",
    "title": "Projeto: Web Scraping - Coletando Informa√ß√µes de Im√≥veis (Viva Real)",
    "section": "Salvando o resultado",
    "text": "Salvando o resultado\nSalvando os dados em um dataframe do Pandas\n\ndf_from_web_scraping = pd.DataFrame(\n    columns = [\n        \"Endere√ßo\", \n        \"T√≠tulo\", \n        \"√Årea\", \n        \"Und. medida √°rea\", \n        \"Qtd. quartos\", \n        \"Qtd. banheiros\", \n        \"Qtd. vaga garagem\", \n        \"Pre√ßo alugel\", \n        \"Pre√ßo condom√≠nio\",\n        \"Anunciante\"\n    ],\n    data   = np.array([      \n        full_adress,\n        full_title,      \n        full_area,       \n        full_area_unit,  \n        full_room,       \n        full_bath,       \n        full_garage,     \n        full_price,      \n        full_price_condo,\n        full_publisher\n    ]).T\n)\n\ndf_from_web_scraping.to_pickle(\"source/imoveis.pkl\")\ndf_from_web_scraping.to_excel(\"source/imoveis.xlsx\", index = False)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projetos",
    "section": "",
    "text": "Os projetos apresentados aqui ser√£o sempre aplicados em bases de dados p√∫blicas. No entanto, utilizarei a mesma abordagem profissional que emprego no meu dia a dia. O objetivo √© que, √† medida que eu criar novos projetos (ou avan√ßar dentro de um deles), eu v√° atualizando esta p√°gina. Portanto, constantemente haver√° atualiza√ß√µes aqui.\nEm caso de d√∫vidas e/ou sugest√µes, sinta-se √† vontade para me contatar via inbox no LinkedIn."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "",
    "text": "A busca por similaridade (Similarity Search) no contexto da Linguagem Natural (NLP) desempenha um papel crucial em diversas aplica√ß√µes, desde a recupera√ß√£o de informa√ß√µes at√© a an√°lise de sentimentos e a recomenda√ß√£o de conte√∫do. Essa abordagem consiste em encontrar itens semelhantes a um item de consulta, onde a semelhan√ßa √© avaliada atrav√©s da dist√¢ncia entre vetores num√©ricos que representam segmentos de texto (embeddings).\n\n\nNos modelos de linguagem de grande escala (LLM) no contexto de NLP, a busca por similaridade √© frequentemente empregada na engenharia de prompts, onde apenas os documentos mais relevantes para responder a uma determinada pergunta s√£o selecionados e passados para o modelo de LLM. Isso ajuda a otimizar o desempenho do modelo, reduzindo a carga computacional e melhorando a precis√£o das respostas fornecidas.\n\n\nPara facilitar a implementa√ß√£o da busca por similaridade, muitas vezes s√£o utilizados bancos de dados e ferramentas otimizadas para esse fim, como o MongoDB cluster. O MongoDB cluster oferece suporte para indexa√ß√£o de dados de texto e consultas avan√ßadas que permitem recuperar documentos com base em sua similaridade com uma consulta espec√≠fica. Essa capacidade √© fundamental para sistemas que lidam com grandes volumes de dados textuais e precisam fornecer respostas r√°pidas e precisas aos usu√°rios.\n\n\nAl√©m do MongoDB cluster, existe o Chroma que √© uma op√ß√£o interessante para realizar buscas por similaridade localmente. Como um banco de dados open source projetado especialmente para suportar opera√ß√µes de busca por similaridade em dados de alta dimensionalidade, como embeddings de texto, o Chroma oferece uma solu√ß√£o eficiente e escal√°vel para realizar tarefas de NLP localmente, sem a necessidade de configurar e gerenciar um ambiente distribu√≠do.\n\n\nDiante da import√¢ncia e relev√¢ncia da busca por similaridade no contexto da NLP, vamos construir um projeto para demonstrar como realizar essa tarefa utilizando o Chroma."
  },
  {
    "objectID": "project3.html#setup-and-constants",
    "href": "project3.html#setup-and-constants",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Setup and Constants",
    "text": "Setup and Constants\nImportando todos os m√≥dulos necess√°rios os quais j√° foram previamente instalados.\n\n#%% Setup\nfrom chromadb.utils           import embedding_functions            # Embbeding\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter # Split\nfrom transformers             import GPT2TokenizerFast              # Tokenization\nimport chromadb                                                     # DB\nimport textract                                                     # load data"
  },
  {
    "objectID": "project3.html#load-data",
    "href": "project3.html#load-data",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Load data",
    "text": "Load data\nAqui, vamos carregar um arquivo de texto arbitr√°rio. Neste exemplo, vamos utilizar o meu curr√≠culo, o mesmo utilizado no projeto https://pedrodubiela95.github.io/portfolio/project1.html.\n\n#%% Load files\nfile_path = \"./source/project1/context/cvPedro.pdf\"\ndoc       = textract.process(file_path)\ndata      = doc.decode('utf-8')"
  },
  {
    "objectID": "project3.html#document-splitting---tokenization",
    "href": "project3.html#document-splitting---tokenization",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Document Splitting - Tokenization",
    "text": "Document Splitting - Tokenization\nNesta etapa segmentamos nosso arquivo de texto em documentos.\n\n#%% Split into documents\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -&gt; int:\n    return len(tokenizer.encode(text))\n\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size    = 100, # Quantidade m√°xima de caracteres por split\n    chunk_overlap = 15,  # Quantidade de caracteres sobrepostos por split\n    length_function = count_tokens,\n)\n\nsplits = text_splitter.create_documents([data])"
  },
  {
    "objectID": "project3.html#embedding",
    "href": "project3.html#embedding",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Embedding",
    "text": "Embedding\nAplicamos o embedding atrav√©s do modelo open-source ‚Äúall-mpnet-base-v2‚Äù\n\n#%% Embbeding\nsentence_transformer_ef = (\n    embedding_functions\n    .SentenceTransformerEmbeddingFunction(\n        model_name=\"all-mpnet-base-v2\"\n        )\n    )"
  },
  {
    "objectID": "project3.html#create-db-and-collection",
    "href": "project3.html#create-db-and-collection",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Create DB and collection",
    "text": "Create DB and collection\nCriamos o banco e a collection, e informamos qual tipo de embedding ser√° utilizado, bem como o tipo de dist√¢ncia utilizado no c√°lculo da similaridade entre os vetores gerados (embeddings).\n\n#%% Local to save DB\nclient = chromadb.PersistentClient(path=\"./\")\nclient.heartbeat()\n\n# Try delete collection\ntry:\n    client.delete_collection(name=\"my_collection\")\nexcept:\n    pass\n\n# Create a collection\ncollection = client.create_collection(\n    name               =\"my_collection\",\n    metadata           = {\"hnsw:space\": \"cosine\"},\n    embedding_function = sentence_transformer_ef\n    )\n\nAdicionamos os documentos na collection.\n\n# Add data\ncollection.add(\n    documents = [s.page_content for s in splits],\n    ids       = [f\"id{i}\" for i in range(len(splits))]\n    )\n\n# Qty documents in collections\n#collection.count()\n#collection.peek()"
  },
  {
    "objectID": "project3.html#similarity-search",
    "href": "project3.html#similarity-search",
    "title": "Projeto: Similarity Search no contexto de Processamento de Linguagem Natural (NLP)",
    "section": "Similarity Search",
    "text": "Similarity Search\nEfetuamos de fato a busca por similaridade.\nExemplo1: Vamos fazer a seguinte pergunta Onde o Pedro mora? e retornar qual segmento de texto contido no curr√≠culo possui maior simililaridade com essa pergunta.\n\n#%% Similarity Search\n\nquery = \"Onde o Pedro mora?\"\nquery_vector = sentence_transformer_ef(query)\nres = collection.query(\n    query_embeddings= query_vector,\n    n_results = 1,\n    include=['distances','embeddings', 'documents', 'metadatas']\n    )\nfirst = res[\"documents\"][0][0]\nprint(first)\n\nPedro Gasparine Dubiela\nAlt√¥nia - PR\n\nhttps://pedrodubiela95.github.io/portfolio/\n\npedrodubielabio@gmail.com\n\n(44) 9-9708-9090\n\n\nExemplo2: Vamos fazer a seguinte pergunta Quais as habildiades do Pedro? e retornar qual segmento de texto contido no curr√≠culo possui maior simililaridade com essa pergunta.\n\nquery = \"Quais as habildiades do Pedro?\"\nquery_vector = sentence_transformer_ef(query)\nres = collection.query(\n    query_embeddings= query_vector,\n    n_results = 1,\n    include=['distances','embeddings', 'documents', 'metadatas']\n    )\nfirst = res[\"documents\"][0][0]\nprint(first)\n\nHabilidades\n&gt;&gt;&gt; Ingl√™s Intermedi√°rio (B1-B2)\n&gt;&gt;&gt; Forte background em matem√°tica e estat√≠stica\n&gt;&gt;&gt; S√≥lidos conhecimentos em ci√™ncia da computa√ß√£o\n&gt;&gt;&gt; An√°lise de dados"
  }
]